CREATE POLICY journal_entries_tenant_isolation ON journal_entries
    FOR ALL TO authenticated_users
    USING (tenant_id = current_setting('app.current_tenant_id')::uuid);

-- Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ñ‚ÐµÐ½Ð°Ð½Ñ‚Ð°
CREATE OR REPLACE FUNCTION set_tenant_context(tenant_uuid UUID)
RETURNS void AS $
BEGIN
    PERFORM set_config('app.current_tenant_id', tenant_uuid::text, false);
END;
$ LANGUAGE plpgsql;
```

### Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¤Ð°Ð·Ñ‹ 4:
- âœ… Cloud-native deployment Ð² GCP/AWS
- âœ… ML-powered fraud detection
- âœ… Real-time analytics Ñ ClickHouse
- âœ… BI dashboard Ñ Plotly
- âœ… Multi-tenant Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°
- âœ… 95% Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð°Ð¼Ð¸ Ð²ÑÐµÐ¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹

---

## ðŸŽ¯ Ð¤Ð°Ð·Ð° 5: ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Go-Live (ÐœÐµÑÑÑ†Ñ‹ 19-24)

### Ð¦ÐµÐ»Ð¸:
- Performance tuning
- Load testing
- Security audit
- User training
- Production deployment

### 5.1 Performance Optimization (ÐœÐµÑÑÑ†Ñ‹ 19-20)

#### Database optimization
```sql
-- performance_optimization.sql

-- ÐŸÐ°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ† Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
CREATE TABLE journal_entries_optimized (
    LIKE journal_entries INCLUDING ALL
) PARTITION BY RANGE (transaction_date);

-- Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¹ Ð½Ð° 2 Ð³Ð¾Ð´Ð° Ð²Ð¿ÐµÑ€ÐµÐ´
DO $
DECLARE
    start_date DATE := '2025-01-01';
    end_date DATE;
    partition_name TEXT;
BEGIN
    FOR i IN 0..23 LOOP
        end_date := start_date + INTERVAL '1 month';
        partition_name := 'journal_entries_' || to_char(start_date, 'YYYY_MM');
        
        EXECUTE format('CREATE TABLE %I PARTITION OF journal_entries_optimized 
                       FOR VALUES FROM (%L) TO (%L)',
                       partition_name, start_date, end_date);
        
        -- Ð˜Ð½Ð´ÐµÐºÑÑ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¸
        EXECUTE format('CREATE INDEX %I ON %I (tenant_id, account_id, transaction_date)',
                       'idx_' || partition_name || '_lookup', partition_name);
        
        start_date := end_date;
    END LOOP;
END $;

-- ÐœÐ°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¹
CREATE MATERIALIZED VIEW account_balances_daily AS
SELECT 
    tenant_id,
    account_id,
    transaction_date,
    sum(debit - credit) OVER (
        PARTITION BY tenant_id, account_id 
        ORDER BY transaction_date 
        ROWS UNBOUNDED PRECEDING
    ) as running_balance,
    sum(debit - credit) as daily_change
FROM journal_entries_optimized
WHERE transaction_date >= CURRENT_DATE - INTERVAL '2 years'
GROUP BY tenant_id, account_id, transaction_date, debit, credit;

CREATE UNIQUE INDEX idx_account_balances_daily_unique 
ON account_balances_daily (tenant_id, account_id, transaction_date);

-- ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹
CREATE OR REPLACE FUNCTION refresh_daily_balances()
RETURNS void AS $
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY account_balances_daily;
END;
$ LANGUAGE plpgsql;

-- ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹
SELECT cron.schedule('refresh-daily-balances', '0 1 * * *', 'SELECT refresh_daily_balances();');
```

#### Caching strategy
```python
# caching/redis_cache.py
from functools import wraps
import redis.asyncio as redis
import json
import hashlib
from typing import Any, Optional, Callable
import asyncio

class DistributedCache:
    def __init__(self, redis_url: str = "redis://redis:6379"):
        self.redis = redis.from_url(redis_url, decode_responses=True)
        self.default_ttl = 3600  # 1 Ñ‡Ð°Ñ
    
    def cache_key(self, prefix: str, *args, **kwargs) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÐºÐ»ÑŽÑ‡Ð° ÐºÑÑˆÐ° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²"""
        key_data = f"{prefix}:{args}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def get(self, key: str) -> Optional[Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ð· ÐºÑÑˆÐ°"""
        try:
            value = await self.redis.get(key)
            if value:
                return json.loads(value)
        except Exception as e:
            logger.error(f"Cache get error: {e}")
        return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÐºÑÑˆ"""
        try:
            ttl = ttl or self.default_ttl
            serialized = json.dumps(value, default=str)
            return await self.redis.setex(key, ttl, serialized)
        except Exception as e:
            logger.error(f"Cache set error: {e}")
            return False
    
    async def delete(self, pattern: str) -> int:
        """Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡Ð¸ Ð¿Ð¾ ÑˆÐ°Ð±Ð»Ð¾Ð½Ñƒ"""
        try:
            keys = await self.redis.keys(pattern)
            if keys:
                return await self.redis.delete(*keys)
            return 0
        except Exception as e:
            logger.error(f"Cache delete error: {e}")
            return 0
    
    def cached(self, ttl: Optional[int] = None, key_prefix: str = "cache"):
        """Ð”ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Ð˜ÑÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ self Ð¸Ð· Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ»ÑŽÑ‡Ð° ÐºÑÑˆÐ°
                cache_args = args[1:] if args and hasattr(args[0], '__class__') else args
                cache_key = self.cache_key(f"{key_prefix}:{func.__name__}", *cache_args, **kwargs)
                
                # ÐŸÐ¾Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð· ÐºÑÑˆÐ°
                cached_result = await self.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # Ð’Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
                result = await func(*args, **kwargs)
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð² ÐºÑÑˆ
                await self.set(cache_key, result, ttl)
                return result
            
            return wrapper
        return decorator

class SmartCache:
    """Ð£Ð¼Ð½Ð¾Ðµ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ invalidation Ð¿Ð¾ Ñ‚ÑÐ³Ð°Ð¼"""
    
    def __init__(self, cache: DistributedCache):
        self.cache = cache
        self.tag_prefix = "tags:"
    
    async def set_with_tags(self, key: str, value: Any, tags: list, ttl: Optional[int] = None):
        """Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ñ Ñ‚ÑÐ³Ð°Ð¼Ð¸ Ð´Ð»Ñ invalidation"""
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ
        await self.cache.set(key, value, ttl)
        
        # Ð¡Ð²ÑÐ·Ð°Ñ‚ÑŒ Ñ Ñ‚ÑÐ³Ð°Ð¼Ð¸
        for tag in tags:
            tag_key = f"{self.tag_prefix}{tag}"
            await self.cache.redis.sadd(tag_key, key)
            if ttl:
                await self.cache.redis.expire(tag_key, ttl + 86400)  # TTL + 1 Ð´ÐµÐ½ÑŒ Ð´Ð»Ñ Ñ‚ÑÐ³Ð¾Ð²
    
    async def invalidate_by_tag(self, tag: str):
        """Ð˜Ð½Ð²Ð°Ð»Ð¸Ð´Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²ÑÐµ ÐºÐ»ÑŽÑ‡Ð¸ Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ñ‹Ð¼ Ñ‚ÑÐ³Ð¾Ð¼"""
        tag_key = f"{self.tag_prefix}{tag}"
        keys = await self.cache.redis.smembers(tag_key)
        
        if keys:
            # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸
            await self.cache.redis.delete(*keys)
            # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ñ‚ÑÐ³
            await self.cache.redis.delete(tag_key)
            
        return len(keys)

# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² ÑÐµÑ€Ð²Ð¸ÑÐ°Ñ…
class AccountService:
    def __init__(self):
        self.cache = DistributedCache()
        self.smart_cache = SmartCache(self.cache)
        self.repository = AccountRepository()
    
    @cache.cached(ttl=1800, key_prefix="accounts")
    async def get_account_by_id(self, tenant_id: str, account_id: str) -> Optional[Account]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‡ÐµÑ‚ Ð¿Ð¾ ID Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼"""
        with TenantContext(tenant_id):
            return await self.repository.get_by_id(account_id)
    
    async def create_account(self, tenant_id: str, account_data: AccountCreate) -> Account:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÑ‡ÐµÑ‚ Ñ Ð¸Ð½Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÐµÐ¹ ÐºÑÑˆÐ°"""
        with TenantContext(tenant_id):
            account = await self.repository.create(account_data)
            
            # Ð˜Ð½Ð²Ð°Ð»Ð¸Ð´Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÑÑˆ
            await self.smart_cache.invalidate_by_tag(f"tenant:{tenant_id}:accounts")
            
            return account
    
    @smart_cache.cached(ttl=300, tags_func=lambda tenant_id: [f"tenant:{tenant_id}:balances"])
    async def get_account_balance(self, tenant_id: str, account_id: str, as_of_date: Optional[date] = None) -> Decimal:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð±Ð°Ð»Ð°Ð½Ñ ÑÑ‡ÐµÑ‚Ð° Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¿Ð¾ Ñ‚ÑÐ³Ð°Ð¼"""
        with TenantContext(tenant_id):
            return await self.repository.get_balance(account_id, as_of_date)
```

### 5.2 Load Testing (ÐœÐµÑÑÑ† 21)

#### Load testing Ñ Locust
```python
# load_testing/locustfile.py
from locust import HttpUser, task, between
import json
import uuid
from datetime import datetime, date
import random

class AccountingSystemUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """ÐÐ²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿ÐµÑ€ÐµÐ´ Ð½Ð°Ñ‡Ð°Ð»Ð¾Ð¼ Ñ‚ÐµÑÑ‚Ð¾Ð²"""
        self.login()
        self.tenant_id = "test-tenant-001"
        self.auth_headers = {
            "Authorization": f"Bearer {self.access_token}",
            "X-Tenant-ID": self.tenant_id,
            "Content-Type": "application/json"
        }
    
    def login(self):
        """ÐÐ²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ"""
        login_data = {
            "username": f"testuser_{random.randint(1, 100)}",
            "password": "testpass123"
        }
        response = self.client.post("/api/v1/auth/login", json=login_data)
        if response.status_code == 200:
            self.access_token = response.json()["access_token"]
        else:
            self.access_token = "dummy-token"
    
    @task(3)
    def get_accounts(self):
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¿Ð¸ÑÐºÐ° ÑÑ‡ÐµÑ‚Ð¾Ð² (Ñ‡Ð°ÑÑ‚Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ)"""
        self.client.get("/api/v1/accounts", headers=self.auth_headers)
    
    @task(2)
    def get_account_balance(self):
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð±Ð°Ð»Ð°Ð½ÑÐ° ÑÑ‡ÐµÑ‚Ð°"""
        account_id = str(uuid.uuid4())  # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ ID
        self.client.get(
            f"/api/v1/accounts/{account_id}/balance",
            headers=self.auth_headers
        )
    
    @task(5)
    def create_transaction(self):
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ (Ð¾ÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°)"""
        transaction_data = {
            "transaction_date": date.today().isoformat(),
            "description": f"Test transaction {random.randint(1, 10000)}",
            "entries": [
                {
                    "account_id": str(uuid.uuid4()),
                    "debit": random.uniform(100, 10000),
                    "credit": 0,
                    "description": "Test debit entry"
                },
                {
                    "account_id": str(uuid.uuid4()),
                    "debit": 0,
                    "credit": random.uniform(100, 10000),
                    "description": "Test credit entry"
                }
            ]
        }
        
        # Ð£Ð±ÐµÐ´Ð¸Ñ‚ÑŒÑÑ Ñ‡Ñ‚Ð¾ Ð´ÐµÐ±ÐµÑ‚ = ÐºÑ€ÐµÐ´Ð¸Ñ‚
        transaction_data["entries"][1]["credit"] = transaction_data["entries"][0]["debit"]
        
        self.client.post(
            "/api/v1/transactions",
            json=transaction_data,
            headers=self.auth_headers
        )
    
    @task(1)
    def get_financial_report(self):
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° (Ñ‚ÑÐ¶ÐµÐ»Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ)"""
        params = {
            "start_date": "2025-01-01",
            "end_date": date.today().isoformat(),
            "report_type": "balance_sheet"
        }
        
        self.client.get(
            "/api/v1/reports/financial",
            params=params,
            headers=self.auth_headers
        )
    
    @task(1)
    def calculate_vat(self):
        """Ð Ð°ÑÑ‡ÐµÑ‚ ÐÐ”Ð¡"""
        vat_data = {
            "net_amount": random.uniform(1000, 50000),
            "is_exempt": random.choice([True, False])
        }
        
        self.client.post(
            "/api/v1/tax/vat/calculate",
            json=vat_data,
            headers=self.auth_headers
        )

class HighLoadUser(HttpUser):
    """ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¾Ð¹ Ð´Ð»Ñ ÑÑ‚Ñ€ÐµÑÑ-Ñ‚ÐµÑÑ‚Ð¾Ð²"""
    wait_time = between(0.1, 0.5)  # Ð‘Ð¾Ð»ÐµÐµ Ð°Ð³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð°Ñ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°
    weight = 1
    
    @task
    def rapid_balance_checks(self):
        """Ð§Ð°ÑÑ‚Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð±Ð°Ð»Ð°Ð½ÑÐ¾Ð²"""
        for _ in range(10):
            account_id = random.choice([
                "550e8400-e29b-41d4-a716-446655440001",
                "550e8400-e29b-41d4-a716-446655440002",
                "550e8400-e29b-41d4-a716-446655440003"
            ])
            self.client.get(f"/api/v1/accounts/{account_id}/balance")

# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ² Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
class LoadTestConfig:
    scenarios = {
        "normal_load": {
            "users": 100,
            "spawn_rate": 5,
            "duration": "10m"
        },
        "peak_load": {
            "users": 500,
            "spawn_rate": 25,
            "duration": "15m"
        },
        "stress_test": {
            "users": 1000,
            "spawn_rate": 50,
            "duration": "30m"
        },
        "endurance_test": {
            "users": 200,
            "spawn_rate": 10,
            "duration": "2h"
        }
    }
```

#### Performance monitoring
```python
# monitoring/performance_monitor.py
import asyncio
import psutil
import time
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List
import aiohttp

@dataclass
class PerformanceMetrics:
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_io: Dict[str, float]
    network_io: Dict[str, float]
    response_times: Dict[str, float]
    error_rates: Dict[str, float]
    active_connections: int

class PerformanceMonitor:
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.thresholds = {
            "cpu_usage": 80.0,
            "memory_usage": 85.0,
            "avg_response_time": 2.0,
            "error_rate": 5.0
        }
        self.alerts_sent = set()
    
    async def collect_system_metrics(self) -> Dict:
        """Ð¡Ð±Ð¾Ñ€ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk_io = psutil.disk_io_counters()
        network_io = psutil.net_io_counters()
        
        return {
            "cpu_usage": cpu_percent,
            "memory_usage": memory.percent,
            "disk_io": {
                "read_bytes": disk_io.read_bytes,
                "write_bytes": disk_io.write_bytes
            },
            "network_io": {
                "bytes_sent": network_io.bytes_sent,
                "bytes_recv": network_io.bytes_recv
            }
        }
    
    async def collect_application_metrics(self) -> Dict:
        """Ð¡Ð±Ð¾Ñ€ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ"""
        # Ð—Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ðº Prometheus metrics endpoint
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:8000/metrics") as response:
                metrics_text = await response.text()
        
        # ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¼ÐµÑ‚Ñ€Ð¸Ðº (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð¾)
        response_times = {}
        error_rates = {}
        active_connections = 0
        
        for line in metrics_text.split('\n'):
            if 'http_request_duration_seconds' in line and 'quantile="0.95"' in line:
                # Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ 95-Ð³Ð¾ Ð¿ÐµÑ€Ñ†ÐµÐ½Ñ‚Ð¸Ð»Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
                value = float(line.split()[-1])
                endpoint = self._extract_endpoint(line)
                response_times[endpoint] = value
            
            elif 'http_requests_total' in line and 'status="5' in line:
                # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð¾ÑˆÐ¸Ð±Ð¾Ðº 5xx
                value = float(line.split()[-1])
                endpoint = self._extract_endpoint(line)
                error_rates[endpoint] = value
        
        return {
            "response_times": response_times,
            "error_rates": error_rates,
            "active_connections": active_connections
        }
    
    async def check_thresholds(self, metrics: PerformanceMetrics):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾Ñ€Ð¾Ð³Ð¾Ð²Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð°Ð»ÐµÑ€Ñ‚Ð¾Ð²"""
        alerts = []
        
        if metrics.cpu_usage > self.thresholds["cpu_usage"]:
            alerts.append(f"High CPU usage: {metrics.cpu_usage:.1f}%")
        
        if metrics.memory_usage > self.thresholds["memory_usage"]:
            alerts.append(f"High memory usage: {metrics.memory_usage:.1f}%")
        
        avg_response_time = sum(metrics.response_times.values()) / len(metrics.response_times) if metrics.response_times else 0
        if avg_response_time > self.thresholds["avg_response_time"]:
            alerts.append(f"High average response time: {avg_response_time:.2f}s")
        
        for alert in alerts:
            alert_hash = hash(alert)
            if alert_hash not in self.alerts_sent:
                await self.send_alert(alert)
                self.alerts_sent.add(alert_hash)
                
                # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð°Ð»ÐµÑ€Ñ‚Ñ‹
                if len(self.alerts_sent) > 100:
                    self.alerts_sent.clear()
    
    async def send_alert(self, message: str):
        """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð°Ð»ÐµÑ€Ñ‚Ð°"""
        alert_data = {
            "severity": "warning",
            "message": message,
            "timestamp": datetime.utcnow().isoformat(),
            "service": "accounting-system"
        }
        
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð² Slack/Teams/Email
        async with aiohttp.ClientSession() as session:
            await session.post(
                "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK",
                json={"text": f"ðŸš¨ Performance Alert: {message}"}
            )
    
    async def run_monitoring(self):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°"""
        while True:
            try:
                system_metrics = await self.collect_system_metrics()
                app_metrics = await self.collect_application_metrics()
                
                metrics = PerformanceMetrics(
                    timestamp=datetime.utcnow(),
                    **system_metrics,
                    **app_metrics
                )
                
                self.metrics_history.append(metrics)
                
                # ÐžÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 1000 Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]
                
                await self.check_thresholds(metrics)
                
            except Exception as e:
                logger.error(f"Performance monitoring error: {e}")
            
            await asyncio.sleep(30)  # Ð¡Ð±Ð¾Ñ€ Ð¼ÐµÑ‚Ñ€Ð¸Ðº ÐºÐ°Ð¶Ð´Ñ‹Ðµ 30 ÑÐµÐºÑƒÐ½Ð´
```

### 5.3 Security Audit (ÐœÐµÑÑÑ† 22)

#### Security checklist
```python
# security/audit_checklist.py
from dataclasses import dataclass
from typing import List, Dict
import asyncio
import re

@dataclass
class SecurityCheck:
    name: str
    description: str
    status: str  # pass, fail, warning
    details: str
    recommendation: str

class SecurityAuditor:
    def __init__(self):
        self.checks: List[SecurityCheck] = []
    
    async def run_full_audit(self) -> List[SecurityCheck]:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð°ÑƒÐ´Ð¸Ñ‚ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸"""
        await asyncio.gather(
            self.check_authentication(),
            self.check_authorization(),
            self.check_input_validation(),
            self.check_data_encryption(),
            self.check_database_security(),
            self.check_api_security(),
            self.check_logging_and_monitoring(),
            self.check_dependency_vulnerabilities()
        )
        
        return self.checks
    
    async def check_authentication(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸"""
        checks = [
            # JWT Ñ‚Ð¾ÐºÐµÐ½Ñ‹
            await self.verify_jwt_security(),
            # ÐŸÐ°Ñ€Ð¾Ð»Ð¸
            await self.check_password_policy(),
            # MFA
            await self.check_mfa_implementation(),
            # Session management
            await self.check_session_security()
        ]
        self.checks.extend(checks)
    
    async def verify_jwt_security(self) -> SecurityCheck:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ JWT Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²"""
        # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ¸
        jwt_algorithm = "HS256"  # ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð· ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸
        
        if jwt_algorithm in ["none", "HS256"]:
            return SecurityCheck(
                name="JWT Algorithm",
                description="JWT signing algorithm verification",
                status="warning" if jwt_algorithm == "HS256" else "fail",
                details=f"Current algorithm: {jwt_algorithm}",
                recommendation="Consider using RS256 for better security"
            )
        
        return SecurityCheck(
            name="JWT Algorithm",
            description="JWT signing algorithm verification", 
            status="pass",
            details=f"Secure algorithm in use: {jwt_algorithm}",
            recommendation=""
        )
    
    async def check_password_policy(self) -> SecurityCheck:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ Ð¿Ð°Ñ€Ð¾Ð»ÐµÐ¹"""
        password_policy = {
            "min_length": 8,
            "require_uppercase": True,
            "require_lowercase": True,
            "require_numbers": True,
            "require_special": True,
            "max_age_days": 90
        }
        
        if password_policy["min_length"] < 12:
            return SecurityCheck(
                name="Password Policy",
                description="Password strength requirements",
                status="warning",
                details=f"Minimum length: {password_policy['min_length']} characters",
                recommendation="Increase minimum password length to 12+ characters"
            )
        
        return SecurityCheck(
            name="Password Policy",
            description="Password strength requirements",
            status="pass", 
            details="Strong password policy enforced",
            recommendation=""
        )
    
    async def check_input_validation(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        # SQL Injection protection
        sql_injection_check = await self.check_sql_injection_protection()
        self.checks.append(sql_injection_check)
        
        # XSS protection
        xss_check = await self.check_xss_protection()
        self.checks.append(xss_check)
        
        # CSRF protection
        csrf_check = await self.check_csrf_protection()
        self.checks.append(csrf_check)
    
    async def check_sql_injection_protection(self) -> SecurityCheck:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹ Ð¾Ñ‚ SQL Ð¸Ð½ÑŠÐµÐºÑ†Ð¸Ð¹"""
        # Ð¡ÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð´Ð° Ð½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
        vulnerable_patterns = [
            r'execute\(.*\+.*\)',  # ÐšÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð°Ñ†Ð¸Ñ Ð² SQL
            r'f".*{.*}.*".*execute',  # f-ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð² SQL
            r'%.*%.*execute'  # Ð¡Ñ‚Ð°Ñ€Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² SQL
        ]
        
        # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»Ñ‹ ÐºÐ¾Ð´Ð°
        vulnerability_found = False
        
        if vulnerability_found:
            return SecurityCheck(
                name="SQL Injection Protection",
                description="Protection against SQL injection attacks",
                status="fail",
                details="Potentially vulnerable code patterns found",
                recommendation="Use only parameterized queries and ORM methods"
            )
        
        return SecurityCheck(
            name="SQL Injection Protection", 
            description="Protection against SQL injection attacks",
            status="pass",
            details="All database queries use parameterized statements",
            recommendation=""
        )
    
    async def check_data_encryption(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑˆÐ¸Ñ„Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        # Encryption at rest
        encryption_at_rest = SecurityCheck(
            name="Data Encryption at Rest",
            description="Sensitive data encryption in database",
            status="pass",  # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ€ÐµÐ°Ð»ÑŒÐ½ÑƒÑŽ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
            details="Database encryption enabled",
            recommendation=""
        )
        self.checks.append(encryption_at_rest)
        
        # Encryption in transit
        encryption_in_transit = SecurityCheck(
            name="Data Encryption in Transit", 
            description="TLS/SSL for all communications",
            status="pass",
            details="TLS 1.3 enforced for all connections",
            recommendation=""
        )
        self.checks.append(encryption_in_transit)
        
        # PII data handling
        pii_handling = await self.check_pii_handling()
        self.checks.append(pii_handling)
    
    async def check_pii_handling(self) -> SecurityCheck:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Georgian Data Protection Law
        gdpr_compliance = True  # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ€ÐµÐ°Ð»ÑŒÐ½ÑƒÑŽ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ
        
        if not gdpr_compliance:
            return SecurityCheck(
                name="PII Data Handling",
                description="Personal data protection compliance", 
                status="fail",
                details="Georgian Data Protection Law compliance issues found",
                recommendation="Implement proper consent management and data retention policies"
            )
        
        return SecurityCheck(
            name="PII Data Handling",
            description="Personal data protection compliance",
            status="pass", 
            details="Compliant with Georgian Data Protection Law",
            recommendation=""
        )
    
    async def generate_security_report(self) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð¿Ð¾ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸"""
        total_checks = len(self.checks)
        passed = len([c for c in self.checks if c.status == "pass"])
        warnings = len([c for c in self.checks if c.status == "warning"])
        failed = len([c for c in self.checks if c.status == "fail"])
        
        report = f"""
# Security Audit Report
Generated: {datetime.utcnow().isoformat()}

## Summary
- Total Checks: {total_checks}
- âœ… Passed: {passed}
- âš ï¸  Warnings: {warnings} 
- âŒ Failed: {failed}

## Security Score: {(passed / total_checks) * 100:.1f}%

## Detailed Results
"""
        
        for check in self.checks:
            status_emoji = {"pass": "âœ…", "warning": "âš ï¸", "fail": "âŒ"}
            report += f"""
### {status_emoji[check.status]} {check.name}
**Description:** {check.description}
**Status:** {check.status.upper()}
**Details:** {check.details}
"""
            if check.recommendation:
                report += f"**Recommendation:** {check.recommendation}\n"
        
        return report
```

### 5.4 User Training Program (ÐœÐµÑÑÑ† 23)

#### Training materials
```python
# training/training_program.py
from dataclasses import dataclass
from typing import List, Dict
from datetime import datetime, timedelta

@dataclass
class TrainingModule:
    id: str
    title: str
    description: str
    duration_minutes: int
    prerequisites: List[str]
    learning_objectives: List[str]
    materials: List[str]
    assessment: Dict

class GeorgianAccountingTrainingProgram:
    def __init__(self):
        self.modules = self.create_training_modules()
        self.user_progress = {}
    
    def create_training_modules(self) -> List[TrainingModule]:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÑƒÐ»Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        return [
            TrainingModule(
                id="basics-001",
                title="ÐžÑÐ½Ð¾Ð²Ñ‹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹: ÐÐ°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ",
                description="Ð˜Ð·ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ° Ð¸ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ",
                duration_minutes=30,
                prerequisites=[],
                learning_objectives=[
                    "ÐžÑÐ²Ð¾Ð¸Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸ÑŽ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ",
                    "ÐŸÐ¾Ð½ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¼ÐµÐ½ÑŽ Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¾Ð²",
                    "ÐÐ°ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ Ð½Ð°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ"
                ],
                materials=[
                    "video: interface_overview.mp4",
                    "pdf: user_interface_guide.pdf", 
                    "interactive: navigation_tour"
                ],
                assessment={
                    "type": "interactive_quiz",
                    "passing_score": 80,
                    "questions": 10
                }
            ),
            
            TrainingModule(
                id="accounting-001", 
                title="ÐŸÐ»Ð°Ð½ ÑÑ‡ÐµÑ‚Ð¾Ð² Ð¸ Ð´Ð²Ð¾Ð¹Ð½Ð°Ñ Ð·Ð°Ð¿Ð¸ÑÑŒ",
                description="Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð¿Ð»Ð°Ð½Ð¾Ð¼ ÑÑ‡ÐµÑ‚Ð¾Ð² Ð¸ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹ Ð´Ð²Ð¾Ð¹Ð½Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸",
                duration_minutes=60,
                prerequisites=["basics-001"],
                learning_objectives=[
                    "ÐŸÐ¾Ð½ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð° ÑÑ‡ÐµÑ‚Ð¾Ð²",
                    "ÐžÑÐ²Ð¾Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹ Ð´Ð²Ð¾Ð¹Ð½Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸", 
                    "ÐÐ°ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÑ‡ÐµÑ‚Ð°"
                ],
                materials=[
                    "video: chart_of_accounts_georgian.mp4",
                    "pdf: georgian_accounting_standards.pdf",
                    "interactive: double_entry_simulator",
                    "template: standard_chart_of_accounts.xlsx"
                ],
                assessment={
                    "type": "practical_exercise",
                    "passing_score": 85,
                    "task": "Create chart of accounts for small Georgian business"
                }
            ),
            
            TrainingModule(
                id="transactions-001",
                title="Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹", 
                description="ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€ÑÐºÐ¸Ð¼Ð¸ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑÐ¼Ð¸",
                duration_minutes=90,
                prerequisites=["accounting-001"],
                learning_objectives=[
                    "Ð¡Ð¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ñ‚Ð¸Ð¿Ñ‹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹",
                    "ÐŸÐ¾Ð½ÑÑ‚ÑŒ workflow ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ",
                    "ÐžÑÐ²Ð¾Ð¸Ñ‚ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ Ñ Ð¼Ð½Ð¾Ð³Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ñ‹Ð¼Ð¸ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸"
                ],
                materials=[
                    "video: transaction_lifecycle.mp4",
                    "pdf: transaction_types_guide.pdf",
                    "interactive: transaction_creation_wizard",
                    "case_study: multi_currency_transactions.pdf"
                ],
                assessment={
                    "type": "scenario_based",
                    "passing_score": 80,
                    "scenarios": [
                        "Process supplier invoice with VAT",
                        "Record salary payments with taxes",
                        "Handle foreign currency purchase"
                    ]
                }
            ),
            
            TrainingModule(
                id="georgian-tax-001",
                title="Ð“Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ°Ñ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°",
                description="Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ ÐÐ”Ð¡, Ð¿Ð¾Ð´Ð¾Ñ…Ð¾Ð´Ð½Ñ‹Ð¼ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð¼ Ð¸ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð² RS.ge",
                duration_minutes=120,
                prerequisites=["transactions-001"],
                learning_objectives=[
                    "ÐžÑÐ²Ð¾Ð¸Ñ‚ÑŒ Ñ€Ð°ÑÑ‡ÐµÑ‚ ÐÐ”Ð¡ 18%",
                    "ÐŸÐ¾Ð½ÑÑ‚ÑŒ Ð¼Ð°Ð»Ñ‹Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ (Ð´Ð¾ 100,000 Ð»Ð°Ñ€Ð¸)",
                    "ÐÐ°ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ Ð¿Ð¾Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð´ÐµÐºÐ»Ð°Ñ€Ð°Ñ†Ð¸Ð¸ Ð² RS.ge",
                    "Ð Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ñ€ÐµÐ²ÐµÑ€Ñ-Ñ‡Ð°Ñ€Ð¶ ÐÐ”Ð¡"
                ],
                materials=[
                    "video: georgian_vat_system.mp4",
                    "pdf: tax_code_summary_2025.pdf",
                    "interactive: vat_calculator_simulator", 
                    "webinar: rs_ge_integration_demo.mp4",
                    "template: monthly_vat_declaration.xlsx"
                ],
                assessment={
                    "type": "certification_exam",
                    "passing_score": 90,
                    "duration_minutes": 60,
                    "topics": ["VAT calculation", "Tax deadlines", "RS.ge procedures"]
                }
            ),
            
            TrainingModule(
                id="payroll-001",
                title="Ð Ð°ÑÑ‡ÐµÑ‚ Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ñ‹ Ð¸ Ð¿ÐµÐ½ÑÐ¸Ð¾Ð½Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° 2+2+2",
                description="ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ñ‹ Ñ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¸Ð¼Ð¸ Ð½Ð°Ð»Ð¾Ð³Ð°Ð¼Ð¸",
                duration_minutes=75,
                prerequisites=["georgian-tax-001"],
                learning_objectives=[
                    "Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ñƒ Ñ Ð¿Ð¾Ð´Ð¾Ñ…Ð¾Ð´Ð½Ñ‹Ð¼ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð¼ 20%",
                    "Ð Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹ 2+2+2 Ð¿ÐµÐ½ÑÐ¸Ð¾Ð½Ð½Ñ‹Ñ… Ð²Ð·Ð½Ð¾ÑÐ¾Ð²",
                    "Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ð½Ñ‹Ðµ Ð²ÐµÐ´Ð¾Ð¼Ð¾ÑÑ‚Ð¸",
                    "ÐŸÐ¾Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ Ð² Ð¿ÐµÐ½ÑÐ¸Ð¾Ð½Ð½Ð¾Ðµ Ð°Ð³ÐµÐ½Ñ‚ÑÑ‚Ð²Ð¾"
                ],
                materials=[
                    "video: georgian_payroll_system.mp4",
                    "pdf: pension_system_guide.pdf",
                    "calculator: salary_tax_calculator",
                    "template: payroll_register.xlsx"
                ],
                assessment={
                    "type": "practical_calculation",
                    "passing_score": 85,
                    "task": "Calculate monthly payroll for 10 employees"
                }
            ),
            
            TrainingModule(
                id="reporting-001", 
                title="Ð¤Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð°Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾ IFRS",
                description="Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ñ… Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð² Ð² ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ñ IFRS",
                duration_minutes=90,
                prerequisites=["payroll-001"],
                learning_objectives=[
                    "Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð±Ð°Ð»Ð°Ð½Ñ Ð¿Ð¾ IFRS",
                    "Ð¡Ð¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑÑ… Ð¸ ÑƒÐ±Ñ‹Ñ‚ÐºÐ°Ñ…",
                    "Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¸ Ð´ÐµÐ½ÐµÐ¶Ð½Ñ‹Ñ… ÑÑ€ÐµÐ´ÑÑ‚Ð²",
                    "ÐŸÐ¾Ð½ÑÑ‚ÑŒ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº Ñ€Ð°ÑÐºÑ€Ñ‹Ñ‚Ð¸ÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸"
                ],
                materials=[
                    "video: ifrs_reporting_overview.mp4",
                    "pdf: ifrs_requirements_2025.pdf",
                    "template: ifrs_financial_statements.xlsx",
                    "case_study: annual_reporting_example.pdf"
                ],
                assessment={
                    "type": "report_generation",
                    "passing_score": 85,
                    "task": "Generate full IFRS financial statements"
                }
            ),
            
            TrainingModule(
                id="advanced-001",
                title="ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¸ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸",
                description="Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ API, Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð¸ Ð²Ð½ÐµÑˆÐ½Ð¸Ð¼Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ð¼Ð¸",
                duration_minutes=60,
                prerequisites=["reporting-001"],
                learning_objectives=[
                    "ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¾Ð²Ð¾Ð´ÐºÐ¸",
                    "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ API Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¹",
                    "Ð Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ð±Ð°Ð½ÐºÐ¾Ð²ÑÐºÐ¸Ð¼Ð¸ Ð²Ñ‹Ð¿Ð¸ÑÐºÐ°Ð¼Ð¸",
                    "ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð°Ð»ÐµÑ€Ñ‚Ñ‹"
                ],
                materials=[
                    "video: automation_features.mp4",
                    "pdf: api_integration_guide.pdf",
                    "hands_on: bank_statement_import",
                    "tutorial: workflow_automation_setup"
                ],
                assessment={
                    "type": "practical_setup",
                    "passing_score": 80,
                    "task": "Configure automated monthly closing process"
                }
            )
        ]
    
    def create_training_schedule(self, user_role: str, available_hours_per_week: int = 5) -> Dict:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        role_modules = {
            "accountant": ["basics-001", "accounting-001", "transactions-001", "georgian-tax-001"],
            "chief_accountant": ["basics-001", "accounting-001", "transactions-001", "georgian-tax-001", "payroll-001", "reporting-001", "advanced-001"],
            "tax_specialist": ["basics-001", "accounting-001", "georgian-tax-001", "reporting-001"],
            "payroll_officer": ["basics-001", "accounting-001", "payroll-001"],
            "auditor": ["basics-001", "accounting-001", "transactions-001", "reporting-001"]
        }
        
        required_modules = role_modules.get(user_role, ["basics-001"])
        
        schedule = []
        current_date = datetime.now()
        weekly_minutes = available_hours_per_week * 60
        
        for module_id in required_modules:
            module = next((m for m in self.modules if m.id == module_id), None)
            if module:
                # Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð° Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¸)
                total_time = module.duration_minutes * 1.5  # +50% Ð½Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒ
                
                # Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¿Ð¾ Ð½ÐµÐ´ÐµÐ»ÑÐ¼
                weeks_needed = max(1, int(total_time / weekly_minutes))
                
                schedule.append({
                    "module": module,
                    "start_date": current_date,
                    "end_date": current_date + timedelta(weeks=weeks_needed),
                    "estimated_hours": total_time / 60
                })
                
                current_date += timedelta(weeks=weeks_needed)
        
        return {
            "user_role": user_role,
            "total_duration_weeks": (current_date - datetime.now()).days // 7,
            "schedule": schedule
        }
    
    def generate_training_materials(self, module_id: str) -> Dict:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ Ð´Ð»Ñ Ð¼Ð¾Ð´ÑƒÐ»Ñ"""
        module = next((m for m in self.modules if m.id == module_id), None)
        if not module:
            return {}
        
        # Ð¡Ð¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ Ð´Ð»Ñ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
        georgian_materials = {
            "georgian-tax-001": {
                "vat_examples": [
                    {
                        "scenario": "ÐŸÑ€Ð¾Ð´Ð°Ð¶Ð° Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð½Ð° 1000 Ð»Ð°Ñ€Ð¸",
                        "calculation": "ÐÐ”Ð¡ = 1000 * 18% = 180 Ð»Ð°Ñ€Ð¸",
                        "total": "1180 Ð»Ð°Ñ€Ð¸ Ñ ÐÐ”Ð¡"
                    },
                    {
                        "scenario": "Ð ÐµÐ²ÐµÑ€Ñ-Ñ‡Ð°Ñ€Ð¶ Ñ Ð·Ð°Ñ€ÑƒÐ±ÐµÐ¶Ð½Ð¾Ð³Ð¾ Ð¿Ð¾ÑÑ‚Ð°Ð²Ñ‰Ð¸ÐºÐ°",
                        "calculation": "ÐÐ”Ð¡ Ðº Ð´Ð¾Ð¿Ð»Ð°Ñ‚Ðµ Ð¸ Ð·Ð°Ñ‡ÐµÑ‚Ñƒ = 500 * 18% = 90 Ð»Ð°Ñ€Ð¸",
                        "note": "ÐžÐ´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð½Ð°Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¸ Ð·Ð°ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐÐ”Ð¡"
                    }
                ],
                "tax_calendar": {
                    "monthly_deadlines": [
                        {"task": "ÐŸÐ¾Ð´Ð°Ñ‡Ð° Ð´ÐµÐºÐ»Ð°Ñ€Ð°Ñ†Ð¸Ð¸ ÐÐ”Ð¡", "deadline": "15 Ñ‡Ð¸ÑÐ»Ð¾"},
                        {"task": "Ð£Ð¿Ð»Ð°Ñ‚Ð° ÐÐ”Ð¡", "deadline": "15 Ñ‡Ð¸ÑÐ»Ð¾"},
                        {"task": "ÐžÑ‚Ñ‡ÐµÑ‚ Ð¿Ð¾ Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ð½Ñ‹Ð¼ Ð½Ð°Ð»Ð¾Ð³Ð°Ð¼", "deadline": "15 Ñ‡Ð¸ÑÐ»Ð¾"}
                    ],
                    "annual_deadlines": [
                        {"task": "Ð”ÐµÐºÐ»Ð°Ñ€Ð°Ñ†Ð¸Ñ Ð¾ Ð´Ð¾Ñ…Ð¾Ð´Ð°Ñ…", "deadline": "31 Ð¼Ð°Ñ€Ñ‚Ð°"},
                        {"task": "ÐÑƒÐ´Ð¸Ñ‚Ð¾Ñ€ÑÐºÐ¸Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚", "deadline": "31 Ð¼Ð°Ñ"}
                    ]
                }
            },
            
            "payroll-001": {
                "salary_examples": [
                    {
                        "gross_salary": 2000,
                        "income_tax_20": 400,
                        "pension_employee_2": 40,
                        "pension_employer_2": 40,
                        "pension_government_2": 40,
                        "net_salary": 1560
                    }
                ],
                "pension_calculator": {
                    "annual_limit": 24000,
                    "monthly_limit": 2000,
                    "note": "Ð“Ð¾ÑÑƒÐ´Ð°Ñ€ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ ÑÐ¾Ñ„Ð¸Ð½Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¾ 24,000 Ð»Ð°Ñ€Ð¸ Ð² Ð³Ð¾Ð´"
                }
            }
        }
        
        return {
            "module": module,
            "georgian_specific": georgian_materials.get(module_id, {}),
            "interactive_elements": [
                f"simulation_{module_id}",
                f"quiz_{module_id}",
                f"calculator_{module_id}"
            ]
        }

class TrainingTracker:
    def __init__(self):
        self.user_progress = {}
        self.completion_certificates = {}
    
    async def track_progress(self, user_id: str, module_id: str, progress_data: Dict):
        """ÐžÑ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        if user_id not in self.user_progress:
            self.user_progress[user_id] = {}
        
        self.user_progress[user_id][module_id] = {
            "started_at": progress_data.get("started_at", datetime.utcnow()),
            "completed_at": progress_data.get("completed_at"),
            "progress_percent": progress_data.get("progress_percent", 0),
            "assessment_score": progress_data.get("assessment_score"),
            "time_spent_minutes": progress_data.get("time_spent_minutes", 0),
            "status": progress_data.get("status", "in_progress")  # not_started, in_progress, completed, failed
        }
        
        # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð²Ñ‹Ð´Ð°Ñ‡Ð° ÑÐµÑ€Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð° Ð¿Ñ€Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸
        if (progress_data.get("status") == "completed" and 
            progress_data.get("assessment_score", 0) >= 80):
            await self.issue_certificate(user_id, module_id)
    
    async def issue_certificate(self, user_id: str, module_id: str):
        """Ð’Ñ‹Ð´Ð°Ñ‚ÑŒ ÑÐµÑ€Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚ Ð¾ Ð¿Ñ€Ð¾Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ð¸"""
        certificate = {
            "certificate_id": str(uuid.uuid4()),
            "user_id": user_id,
            "module_id": module_id,
            "issued_at": datetime.utcnow(),
            "valid_until": datetime.utcnow() + timedelta(days=365),
            "verification_url": f"https://accounting-system.ge/certificates/verify/{certificate['certificate_id']}"
        }
        
        self.completion_certificates[certificate["certificate_id"]] = certificate
        
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŽ
        await self.send_certificate_notification(user_id, certificate)
    
    def generate_progress_report(self, user_id: str) -> Dict:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        user_data = self.user_progress.get(user_id, {})
        
        total_modules = len(user_data)
        completed_modules = len([m for m in user_data.values() if m["status"] == "completed"])
        in_progress_modules = len([m for m in user_data.values() if m["status"] == "in_progress"])
        
        average_score = 0
        if completed_modules > 0:
            scores = [m["assessment_score"] for m in user_data.values() 
                     if m["status"] == "completed" and m["assessment_score"]]
            average_score = sum(scores) / len(scores) if scores else 0
        
        return {
            "user_id": user_id,
            "completion_rate": (completed_modules / total_modules * 100) if total_modules > 0 else 0,
            "modules_completed": completed_modules,
            "modules_in_progress": in_progress_modules,
            "average_assessment_score": average_score,
            "total_time_spent_hours": sum(m["time_spent_minutes"] for m in user_data.values()) / 60,
            "certificates_earned": len([c for c in self.completion_certificates.values() 
                                       if c["user_id"] == user_id]),
            "next_recommended_modules": self.get_next_modules(user_id)
        }
```

### 5.5 Production Deployment (ÐœÐµÑÑÑ† 24)

#### Blue-Green Deployment Strategy
```yaml
# deployment/blue-green-deployment.yml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: accounting-system
  namespace: accounting-production
spec:
  replicas: 10
  strategy:
    blueGreen:
      # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Blue-Green deployment
      activeService: accounting-active
      previewService: accounting-preview
      autoPromotionEnabled: false
      scaleDownDelaySeconds: 30
      prePromotionAnalysis:
        templates:
        - templateName: success-rate
        args:
        - name: service-name
          value: accounting-preview
      postPromotionAnalysis:
        templates:
        - templateName: success-rate
        - templateName: latency
        args:
        - name: service-name
          value: accounting-active
      previewReplicaCount: 2
      activeMetadata:
        labels:
          version: stable
      previewMetadata:
        labels:
          version: preview
  selector:
    matchLabels:
      app: accounting-system
  template:
    metadata:
      labels:
        app: accounting-system
    spec:
      containers:
      - name: accounting-api
        image: accounting-system:v2.0.0
        ports:
        - containerPort: 8000
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        
---
# Analysis Templates Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: success-rate
spec:
  args:
  - name: service-name
  metrics:
  - name: success-rate
    provider:
      prometheus:
        address: http://prometheus.monitoring.svc.cluster.local:9090
        query: |
          sum(irate(
            http_requests_total{job="{{args.service-name}}",status!~"5.."}[2m]
          )) /
          sum(irate(
            http_requests_total{job="{{args.service-name}}"}[2m]
          )) * 100
    successCondition: result[0] >= 95
    interval: 30s
    count: 5
    failureLimit: 2

---
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate  
metadata:
  name: latency
spec:
  args:
  - name: service-name
  metrics:
  - name: latency-95th
    provider:
      prometheus:
        address: http://prometheus.monitoring.svc.cluster.local:9090
        query: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{job="{{args.service-name}}"}[2m]))
            by (le)
          ) * 1000
    successCondition: result[0] <= 2000  # 2 ÑÐµÐºÑƒÐ½Ð´Ñ‹
    interval: 30s
    count: 5
    failureLimit: 2
```

#### Production checklist
```python
# deployment/production_checklist.py
from dataclasses import dataclass
from typing import List, Dict, Optional
from enum import Enum
import asyncio

class CheckStatus(Enum):
    PENDING = "pending"
    PASSED = "passed" 
    FAILED = "failed"
    WARNING = "warning"

@dataclass
class ProductionCheck:
    name: str
    description: str
    status: CheckStatus
    details: Optional[str] = None
    blocker: bool = False  # Ð‘Ð»Ð¾ÐºÐ¸Ñ€ÑƒÐµÑ‚ Ð»Ð¸ Ð´ÐµÐ¿Ð»Ð¾Ð¹

class ProductionReadinessChecker:
    def __init__(self):
        self.checks: List[ProductionCheck] = []
    
    async def run_all_checks(self) -> Dict:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ðº production"""
        await asyncio.gather(
            self.check_infrastructure(),
            self.check_database(),
            self.check_security(),
            self.check_monitoring(),
            self.check_backup(),
            self.check_performance(),
            self.check_compliance(),
            self.check_documentation()
        )
        
        # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        total = len(self.checks)
        passed = len([c for c in self.checks if c.status == CheckStatus.PASSED])
        failed = len([c for c in self.checks if c.status == CheckStatus.FAILED])
        warnings = len([c for c in self.checks if c.status == CheckStatus.WARNING])
        blockers = len([c for c in self.checks if c.status == CheckStatus.FAILED and c.blocker])
        
        return {
            "ready_for_production": blockers == 0,
            "total_checks": total,
            "passed": passed,
            "failed": failed,
            "warnings": warnings,
            "blockers": blockers,
            "checks": self.checks
        }
    
    async def check_infrastructure(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹"""
        checks = [
            ProductionCheck(
                "Kubernetes Cluster Health",
                "Verify K8s cluster is healthy and has sufficient resources",
                await self._check_k8s_health(),
                blocker=True
            ),
            ProductionCheck(
                "Load Balancer Configuration", 
                "Verify load balancer is properly configured",
                await self._check_load_balancer(),
                blocker=True
            ),
            ProductionCheck(
                "SSL/TLS Certificates",
                "Verify SSL certificates are valid and not expiring soon",
                await self._check_ssl_certificates(),
                blocker=True
            ),
            ProductionCheck(
                "DNS Configuration",
                "Verify DNS records are correctly configured", 
                await self._check_dns(),
                blocker=True
            )
        ]
        self.checks.extend(checks)
    
    async def check_database(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        checks = [
            ProductionCheck(
                "Database Connectivity",
                "Verify application can connect to production database",
                await self._check_db_connectivity(),
                blocker=True
            ),
            ProductionCheck(
                "Database Performance",
                "Verify database performance meets requirements",
                await self._check_db_performance(),
                blocker=False
            ),
            ProductionCheck(
                "Database Backup",
                "Verify automated backups are configured and working",
                await self._check_db_backup(),
                blocker=True
            ),
            ProductionCheck(
                "Database Security",
                "Verify database security settings",
                await self._check_db_security(),
                blocker=True
            )
        ]
        self.checks.extend(checks)
    
    async def check_compliance(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼"""
        checks = [
            ProductionCheck(
                "Georgian Tax Compliance",
                "Verify compliance with Georgian tax regulations",
                await self._check_georgian_tax_compliance(),
                blocker=True
            ),
            ProductionCheck(
                "IFRS Compliance", 
                "Verify IFRS accounting standards compliance",
                await self._check_ifrs_compliance(),
                blocker=True
            ),
            ProductionCheck(
                "Data Privacy Compliance",
                "Verify Georgian data protection law compliance",
                await self._check_data_privacy(),
                blocker=True
            ),
            ProductionCheck(
                "Audit Trail Completeness",
                "Verify complete audit trail implementation", 
                await self._check_audit_trail(),
                blocker=True
            )
        ]
        self.checks.extend(checks)
    
    async def _check_georgian_tax_compliance(self) -> CheckStatus:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¼Ñƒ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ð¼Ñƒ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ñƒ"""
        compliance_items = [
            "VAT rate 18% correctly configured",
            "Monthly VAT reporting implemented", 
            "100,000 GEL threshold monitoring",
            "RS.ge integration working",
            "Reverse-charge VAT handling",
            "Income tax 20% calculation",
            "Pension system 2+2+2 implemented"
        ]
        
        # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ - Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÑ‚ÑŒ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿ÑƒÐ½ÐºÑ‚
        all_compliant = True  # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
        
        return CheckStatus.PASSED if all_compliant else CheckStatus.FAILED
    
    async def generate_go_live_report(self) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ Ð³Ð¾Ñ‚Ð¾Ð²Ð½Ð¾ÑÑ‚Ð¸ Ðº Ð·Ð°Ð¿ÑƒÑÐºÑƒ"""
        results = await self.run_all_checks()
        
        report = f"""
# Production Readiness Report
Generated: {datetime.utcnow().isoformat()}

## Executive Summary
{'âœ… READY FOR PRODUCTION' if results['ready_for_production'] else 'âŒ NOT READY FOR PRODUCTION'}

## Statistics
- Total Checks: {results['total_checks']}
- Passed: {results['passed']} âœ…
- Failed: {results['failed']} âŒ
- Warnings: {results['warnings']} âš ï¸
- Blockers: {results['blockers']} ðŸš«

## Readiness Score: {(results['passed'] / results['total_checks']) * 100:.1f}%

## Critical Issues
"""
        
        blockers = [c for c in self.checks if c.status == CheckStatus.FAILED and c.blocker]
        if blockers:
            report += "The following issues MUST be resolved before production deployment:\n"
            for check in blockers:
                report += f"- âŒ {check.name}: {check.description}\n"
        else:
            report += "No critical blocking issues found.\n"
        
        report += "\n## All Checks Details\n"
        for check in self.checks:
            status_emoji = {
                CheckStatus.PASSED: "âœ…",
                CheckStatus.FAILED: "âŒ", 
                CheckStatus.WARNING: "âš ï¸",
                CheckStatus.PENDING: "â³"
            }
            
            blocker_flag = " ðŸš« BLOCKER" if check.blocker and check.status == CheckStatus.FAILED else ""
            report += f"- {status_emoji[check.status]} {check.name}{blocker_flag}\n"
            if check.details:
                report += f"  Details: {check.details}\n"
        
        return report

# Go-Live Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ð°
class GoLiveCoordinator:
    def __init__(self):
        self.readiness_checker = ProductionReadinessChecker()
        self.deployment_steps = self.create_deployment_steps()
    
    def create_deployment_steps(self) -> List[Dict]:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð´ÐµÐ¿Ð»Ð¾Ñ"""
        return [
            {
                "step": 1,
                "name": "Pre-deployment verification",
                "description": "Run final production readiness checks",
                "duration_minutes": 30,
                "rollback_possible": True
            },
            {
                "step": 2, 
                "name": "Database migration",
                "description": "Apply production database migrations",
                "duration_minutes": 60,
                "rollback_possible": True
            },
            {
                "step": 3,
                "name": "Blue-green deployment start", 
                "description": "Deploy new version to preview environment",
                "duration_minutes": 20,
                "rollback_possible": True
            },
            {
                "step": 4,
                "name": "Integration testing",
                "description": "Run integration tests against preview environment",
                "duration_minutes": 45,
                "rollback_possible": True
            },
            {
                "step": 5,
                "name": "Traffic switchover",
                "description": "Switch traffic from blue to green environment", 
                "duration_minutes": 5,
                "rollback_possible": True
            },
            {
                "step": 6,
                "name": "Post-deployment verification",
                "description": "Verify all systems operational",
                "duration_minutes": 30,
                "rollback_possible": True
            },
            {
                "step": 7,
                "name": "Monitoring setup",
                "description": "Enable production monitoring and alerting",
                "duration_minutes": 15,
                "rollback_possible": False
            },
            {
                "step": 8,
                "name": "User notification",
                "description": "Notify users about system availability",
                "duration_minutes": 10,
                "rollback_possible": False
            }
        ]
    
    async def execute_go_live(self) -> Dict:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ñƒ go-live"""
        start_time = datetime.utcnow()
        results = []
        
        # ÐŸÑ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°
        readiness_report = await self.readiness_checker.run_all_checks()
        if not readiness_report["ready_for_production"]:
            return {
                "success": False,
                "error": "System not ready for production",
                "readiness_report": readiness_report
            }
        
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ ÑˆÐ°Ð³Ð¾Ð² Ð´ÐµÐ¿Ð»Ð¾Ñ
        for step in self.deployment_steps:
            step_start = datetime.utcnow()
            
            try:
                success = await self.execute_deployment_step(step)
                step_result = {
                    "step": step["step"],
                    "name": step["name"], 
                    "success": success,
                    "start_time": step_start,
                    "duration": (datetime.utcnow() - step_start).total_seconds()
                }
                
                if not success:
                    step_result["error"] = f"Step {step['step']} failed"
                    results.append(step_result)
                    
                    # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ rollback ÐµÑÐ»Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶ÐµÐ½
                    if step["rollback_possible"]:
                        await self.rollback_deployment(step["step"])
                    
                    return {
                        "success": False,
                        "failed_step": step["step"],
                        "results": results
                    }
                
                results.append(step_result)
                
            except Exception as e:
                results.append({
                    "step": step["step"],
                    "name": step["name"],
                    "success": False, 
                    "error": str(e),
                    "start_time": step_start,
                    "duration": (datetime.utcnow() - step_start).total_seconds()
                })
                
                # Rollback Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ
                if step["rollback_possible"]:
                    await self.rollback_deployment(step["step"])
                
                return {
                    "success": False,
                    "failed_step": step["step"],
                    "exception": str(e),
                    "results": results
                }
        
        total_duration = (datetime.utcnow() - start_time).total_seconds()
        
        return {
            "success": True,
            "total_duration_seconds": total_duration,
            "results": results,
            "go_live_time": start_time.isoformat()
        }
```

### Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¤Ð°Ð·Ñ‹ 5:
- âœ… Performance optimization Ñ 2x ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÐµÐ¼ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
- âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾Ðµ load testing Ð´Ð¾ 1000+ concurrent users
- âœ… Security audit Ñ 95%+ security score
- âœ… ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 50+ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
- âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ñ‹Ð¹ production deployment
- âœ… 99.9% uptime Ð² Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¼ÐµÑÑÑ† Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹

---

## ðŸ“Š Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸

### Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ:
- **ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ**: Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð² 3x Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ legacy ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹
- **ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚ÑŒ**: ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð´Ð¾ 10,000 Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
- **Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ**: 99.95% uptime SLA
- **Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ**: Ð¡Ð¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð²ÑÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð°Ð¼ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸

### Ð‘Ð¸Ð·Ð½ÐµÑ-Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹:
- **Ð¡Ð¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼**: 100% compliance Ñ Georgian Tax Code Ð¸ IFRS
- **Ð’Ñ€ÐµÐ¼Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸**: Ð¡Ð¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð½Ð° 70%
- **ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ**: 80% Ñ€ÑƒÑ‚Ð¸Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹
- **ROI**: ÐžÐºÑƒÐ¿Ð°ÐµÐ¼Ð¾ÑÑ‚ÑŒ Ð·Ð° 18 Ð¼ÐµÑÑÑ†ÐµÐ²

### Ð¤Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ð¸:
- **Ð¡Ð½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ñ€Ð°ÑÑ…Ð¾Ð´Ð¾Ð²**: 40%
- **Ð¡Ð¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð°**: Ñ 10 Ð´Ð¾ 2 Ð´Ð½ÐµÐ¹
- **Ð£Ð¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ðµ Ð¾ÑˆÐ¸Ð±Ð¾Ðº**: Ð½Ð° 90%
- **Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð½Ð° Ð°ÑƒÐ´Ð¸Ñ‚Ðµ**: 50% Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¾Ð²

---

## ðŸ“ˆ ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑƒÑÐ¿ÐµÑ…Ð° Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸

### Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ KPI:

| ÐœÐµÑ‚Ñ€Ð¸ÐºÐ° | Ð”Ð¾ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ | ÐŸÐ¾ÑÐ»Ðµ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ | Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ |
|---------|-------------|----------------|-----------|
| Ð’Ñ€ÐµÐ¼Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° API | 3.2Ñ | 0.8Ñ | 75% |
| ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ½Ð°Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ | 100 TPS | 1000 TPS | 900% |
| Ð’Ñ€ÐµÐ¼Ñ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ | 4 Ñ‡Ð°ÑÐ° | 15 Ð¼Ð¸Ð½ÑƒÑ‚ | 94% |
| ÐŸÐ¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð°Ð¼Ð¸ | 25% | 95% | 280% |
| Ð’Ñ€ÐµÐ¼Ñ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ | 4 Ñ‡Ð°ÑÐ° | 5 Ð¼Ð¸Ð½ÑƒÑ‚ | 98% |

### Ð‘Ð¸Ð·Ð½ÐµÑ KPI:

| ÐœÐµÑ‚Ñ€Ð¸ÐºÐ° | Ð”Ð¾ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ | ÐŸÐ¾ÑÐ»Ðµ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ | Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ |
|---------|-------------|----------------|-----------|
| Ð’Ñ€ÐµÐ¼Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° | 2 Ð´Ð½Ñ | 30 Ð¼Ð¸Ð½ÑƒÑ‚ | 97% |
| ÐžÑˆÐ¸Ð±ÐºÐ¸ Ð² Ñ€Ð°ÑÑ‡ÐµÑ‚Ð°Ñ… | 5% | 0.5% | 90% |
| Ð’Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ | 40 Ñ‡Ð°ÑÐ¾Ð² | 12 Ñ‡Ð°ÑÐ¾Ð² | 70% |
| Ð£Ð´Ð¾Ð²Ð»ÐµÑ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ | 6/10 | 9/10 | 50% |
| Ð¡Ð¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð½Ð¾Ñ€Ð¼Ð°Ð¼ | 85% | 100% | 18% |

---

## ðŸ”„ Post-Migration Support Ð¸ Continuous Improvement

### 6.1 Ongoing Support Structure

#### Support Tiers
```python
# support/support_structure.py
from enum import Enum
from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime, timedelta

class SupportTier(Enum):
    L1_BASIC = "l1_basic"           # Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
    L2_TECHNICAL = "l2_technical"   # Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°
    L3_EXPERT = "l3_expert"         # Ð­ÐºÑÐ¿ÐµÑ€Ñ‚Ð½Ð°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°
    L4_VENDOR = "l4_vendor"         # ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð²ÐµÐ½Ð´Ð¾Ñ€Ð°

class Priority(Enum):
    CRITICAL = "critical"     # Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
    HIGH = "high"            # Ð¡ÐµÑ€ÑŒÐµÐ·Ð½Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð½Ð° Ð±Ð¸Ð·Ð½ÐµÑ
    MEDIUM = "medium"        # Ð£Ð¼ÐµÑ€ÐµÐ½Ð½Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ
    LOW = "low"              # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ

@dataclass
class SupportTicket:
    id: str
    priority: Priority
    tier: SupportTier
    title: str
    description: str
    user_id: str
    created_at: datetime
    sla_response_time: timedelta
    sla_resolution_time: timedelta
    assigned_to: Optional[str] = None
    status: str = "open"
    
class SupportSLAMatrix:
    """ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ð° SLA Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð° Ð¸ ÑƒÑ€Ð¾Ð²Ð½Ñ"""
    
    SLA_MATRIX = {
        (Priority.CRITICAL, SupportTier.L1_BASIC): {
            "response_time": timedelta(minutes=15),
            "resolution_time": timedelta(hours=1)
        },
        (Priority.HIGH, SupportTier.L1_BASIC): {
            "response_time": timedelta(hours=1),
            "resolution_time": timedelta(hours=4)
        },
        (Priority.MEDIUM, SupportTier.L1_BASIC): {
            "response_time": timedelta(hours=4),
            "resolution_time": timedelta(hours=24)
        },
        (Priority.LOW, SupportTier.L1_BASIC): {
            "response_time": timedelta(hours=24),
            "resolution_time": timedelta(days=3)
        },
        
        # L2 Technical Support
        (Priority.CRITICAL, SupportTier.L2_TECHNICAL): {
            "response_time": timedelta(minutes=30),
            "resolution_time": timedelta(hours=2)
        },
        (Priority.HIGH, SupportTier.L2_TECHNICAL): {
            "response_time": timedelta(hours=2),
            "resolution_time": timedelta(hours=8)
        },
        
        # L3 Expert Support  
        (Priority.CRITICAL, SupportTier.L3_EXPERT): {
            "response_time": timedelta(hours=1),
            "resolution_time": timedelta(hours=4)
        },
        (Priority.HIGH, SupportTier.L3_EXPERT): {
            "response_time": timedelta(hours=4),
            "resolution_time": timedelta(days=1)
        }
    }
    
    @classmethod
    def get_sla(cls, priority: Priority, tier: SupportTier) -> dict:
        return cls.SLA_MATRIX.get((priority, tier), {
            "response_time": timedelta(days=1),
            "resolution_time": timedelta(days=5)
        })

class GeorgianAccountingSupportTeam:
    """Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸ Ð´Ð»Ñ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¹ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€Ð¸Ð¸"""
    
    def __init__(self):
        self.team_members = {
            "l1_basic": [
                {
                    "name": "Nino Kacharava", 
                    "specialization": "User Interface, Basic Operations",
                    "languages": ["Georgian", "English"],
                    "availability": "09:00-18:00 GMT+4"
                },
                {
                    "name": "Giorgi Mchedlishvili",
                    "specialization": "Reports, Data Entry", 
                    "languages": ["Georgian", "English", "Russian"],
                    "availability": "09:00-18:00 GMT+4"
                }
            ],
            "l2_technical": [
                {
                    "name": "Tamar Dvali",
                    "specialization": "Georgian Tax System, VAT, RS.ge Integration",
                    "certifications": ["Georgian Tax Consultant", "IFRS"],
                    "languages": ["Georgian", "English"],
                    "availability": "08:00-20:00 GMT+4"
                },
                {
                    "name": "Levan Tsintsadze", 
                    "specialization": "Payroll, Pension System 2+2+2",
                    "certifications": ["Payroll Specialist", "Labor Law"],
                    "languages": ["Georgian", "English"],
                    "availability": "08:00-20:00 GMT+4"
                }
            ],
            "l3_expert": [
                {
                    "name": "Ana Khvedelidze",
                    "specialization": "IFRS Compliance, Financial Reporting", 
                    "certifications": ["ACCA", "IFRS Specialist", "CPA"],
                    "languages": ["Georgian", "English"],
                    "availability": "24/7 on-call"
                },
                {
                    "name": "Vakhtang Asatiani",
                    "specialization": "System Architecture, Performance",
                    "certifications": ["AWS Solutions Architect", "Kubernetes"],
                    "languages": ["Georgian", "English"],
                    "availability": "24/7 on-call"
                }
            ]
        }
    
    def escalate_ticket(self, ticket: SupportTicket) -> SupportTicket:
        """Ð­ÑÐºÐ°Ð»Ð°Ñ†Ð¸Ñ Ñ‚Ð¸ÐºÐµÑ‚Ð° Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ"""
        escalation_map = {
            SupportTier.L1_BASIC: SupportTier.L2_TECHNICAL,
            SupportTier.L2_TECHNICAL: SupportTier.L3_EXPERT,
            SupportTier.L3_EXPERT: SupportTier.L4_VENDOR
        }
        
        new_tier = escalation_map.get(ticket.tier)
        if new_tier:
            new_sla = SupportSLAMatrix.get_sla(ticket.priority, new_tier)
            ticket.tier = new_tier
            ticket.sla_response_time = new_sla["response_time"]
            ticket.sla_resolution_time = new_sla["resolution_time"] 
            ticket.assigned_to = None  # ÐŸÐµÑ€ÐµÐ½Ð°Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÑŒ
            
        return ticket
```

### 6.2 Continuous Monitoring Ð¸ Improvement

#### Monitoring Dashboard
```python
# monitoring/business_dashboard.py
from dataclasses import dataclass
from typing import Dict, List
from datetime import datetime, timedelta
import asyncio

@dataclass
class BusinessMetric:
    name: str
    current_value: float
    target_value: float
    trend: str  # up, down, stable
    unit: str
    category: str

class BusinessMetricsDashboard:
    def __init__(self):
        self.metrics_collectors = {
            "user_adoption": self.collect_user_adoption_metrics,
            "system_performance": self.collect_performance_metrics,
            "business_value": self.collect_business_value_metrics,
            "compliance": self.collect_compliance_metrics
        }
    
    async def collect_user_adoption_metrics(self) -> List[BusinessMetric]:
        """ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼Ð¸"""
        return [
            BusinessMetric(
                "Active Daily Users",
                85.5,  # Ð˜Ð· Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸
                90.0,
                "up",
                "percentage",
                "user_adoption"
            ),
            BusinessMetric(
                "Feature Utilization Rate",
                72.3,
                80.0,
                "up", 
                "percentage",
                "user_adoption"
            ),
            BusinessMetric(
                "User Satisfaction Score",
                8.7,
                9.0,
                "stable",
                "score",
                "user_adoption"
            ),
            BusinessMetric(
                "Training Completion Rate",
                94.2,
                95.0,
                "up",
                "percentage", 
                "user_adoption"
            )
        ]
    
    async def collect_business_value_metrics(self) -> List[BusinessMetric]:
        """ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð±Ð¸Ð·Ð½ÐµÑ-Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸"""
        return [
            BusinessMetric(
                "Time to Close Books",
                2.1,  # Ð´Ð½Ð¸
                2.0,
                "down",  # ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ
                "days",
                "business_value"
            ),
            BusinessMetric(
                "Error Rate in Reports",
                0.3,  # Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚
                0.5,
                "down",
                "percentage",
                "business_value"
            ),
            BusinessMetric(
                "Audit Preparation Time",
                65.0,  # Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð¾Ñ‚ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
                60.0,
                "down",
                "percentage_reduction",
                "business_value"
            ),
            BusinessMetric(
                "Compliance Score", 
                98.5,
                99.0,
                "up",
                "percentage",
                "business_value"
            )
        ]
    
    async def collect_compliance_metrics(self) -> List[BusinessMetric]:
        """ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼"""
        return [
            BusinessMetric(
                "Georgian Tax Compliance",
                100.0,
                100.0,
                "stable",
                "percentage",
                "compliance"
            ),
            BusinessMetric(
                "IFRS Compliance",
                97.8,
                99.0,
                "up",
                "percentage", 
                "compliance"
            ),
            BusinessMetric(
                "Data Privacy Compliance",
                99.2,
                100.0,
                "up",
                "percentage",
                "compliance"
            ),
            BusinessMetric(
                "Audit Trail Completeness",
                100.0,
                100.0,
                "stable",
                "percentage",
                "compliance"
            )
        ]
    
    async def generate_executive_summary(self) -> Dict:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ executive summary Ð´Ð»Ñ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð°"""
        all_metrics = []
        for collector in self.metrics_collectors.values():
            metrics = await collector()
            all_metrics.extend(metrics)
        
        # Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼
        categories = {}
        for metric in all_metrics:
            if metric.category not in categories:
                categories[metric.category] = []
            categories[metric.category].append(metric)
        
        # Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ð±Ñ‰Ð¸Ñ… Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÐµÐ¹
        total_metrics = len(all_metrics)
        on_target = len([m for m in all_metrics if m.current_value >= m.target_value])
        needs_attention = len([m for m in all_metrics if m.current_value < m.target_value * 0.9])
        
        return {
            "summary": {
                "overall_health": "Good" if on_target / total_metrics > 0.8 else "Needs Attention",
                "metrics_on_target": f"{on_target}/{total_metrics}",
                "metrics_needing_attention": needs_attention,
                "generated_at": datetime.utcnow().isoformat()
            },
            "categories": categories,
            "key_insights": await self.generate_insights(all_metrics),
            "recommendations": await self.generate_recommendations(all_metrics)
        }
    
    async def generate_insights(self, metrics: List[BusinessMetric]) -> List[str]:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ð¸Ð½ÑÐ°Ð¹Ñ‚Ð¾Ð²"""
        insights = []
        
        # ÐÐ°Ð¹Ñ‚Ð¸ Ñ‚Ð¾Ð¿ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ
        improving_metrics = [m for m in metrics if m.trend == "up" and m.current_value >= m.target_value]
        if improving_metrics:
            best_metric = max(improving_metrics, key=lambda x: x.current_value / x.target_value)
            insights.append(f"Excellent progress in {best_metric.name}: {best_metric.current_value}{best_metric.unit}")
        
        # ÐÐ°Ð¹Ñ‚Ð¸ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð´Ð»Ñ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ
        attention_metrics = [m for m in metrics if m.current_value < m.target_value * 0.9]
        if attention_metrics:
            worst_metric = min(attention_metrics, key=lambda x: x.current_value / x.target_value)
            insights.append(f"Needs attention: {worst_metric.name} at {worst_metric.current_value}{worst_metric.unit} (target: {worst_metric.target_value})")
        
        # ÐžÐ±Ñ‰Ð¸Ðµ Ñ‚Ñ€ÐµÐ½Ð´Ñ‹
        user_metrics = [m for m in metrics if m.category == "user_adoption"]
        if user_metrics:
            avg_satisfaction = sum(m.current_value for m in user_metrics if "satisfaction" in m.name.lower()) / len([m for m in user_metrics if "satisfaction" in m.name.lower()])
            if avg_satisfaction > 8.5:
                insights.append(f"High user satisfaction maintained at {avg_satisfaction:.1f}/10")
        
        return insights

class ContinuousImprovementEngine:
    """Ð”Ð²Ð¸Ð³Ð°Ñ‚ÐµÐ»ÑŒ Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ñ‹Ñ… ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹"""
    
    def __init__(self):
        self.improvement_backlog = []
        self.feedback_analyzer = UserFeedbackAnalyzer()
    
    async def analyze_improvement_opportunities(self) -> List[Dict]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ"""
        opportunities = []
        
        # ÐÐ½Ð°Ð»Ð¸Ð· Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        perf_opportunities = await self.analyze_performance_bottlenecks()
        opportunities.extend(perf_opportunities)
        
        # ÐÐ½Ð°Ð»Ð¸Ð· Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
        feedback_opportunities = await self.analyze_user_feedback()
        opportunities.extend(feedback_opportunities)
        
        # ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð´Ð¾Ð»Ð³Ð°
        tech_debt_opportunities = await self.analyze_technical_debt()
        opportunities.extend(tech_debt_opportunities)
        
        # ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ
        opportunities.sort(key=lambda x: x['impact_score'], reverse=True)
        
        return opportunities
    
    async def analyze_performance_bottlenecks(self) -> List[Dict]:
        """ÐÐ½Ð°Ð»Ð¸Ð· ÑƒÐ·ÐºÐ¸Ñ… Ð¼ÐµÑÑ‚ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸"""
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°
        slow_endpoints = [
            {"endpoint": "/api/v1/reports/financial", "avg_response_time": 3.2},
            {"endpoint": "/api/v1/transactions/search", "avg_response_time": 2.1}
        ]
        
        opportunities = []
        for endpoint in slow_endpoints:
            if endpoint["avg_response_time"] > 2.0:
                opportunities.append({
                    "type": "performance",
                    "title": f"Optimize {endpoint['endpoint']} performance",
                    "description": f"Current response time: {endpoint['avg_response_time']}s, target: <1.0s",
                    "impact_score": 85,
                    "effort_estimate": "2-3 weeks",
                    "category": "performance_optimization"
                })
        
        return opportunities
    
    async def analyze_user_feedback(self) -> List[Dict]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹"""
        # Ð¡Ð±Ð¾Ñ€ Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð² Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
        common_requests = [
            {
                "request": "Bulk transaction import",
                "frequency": 23,
                "users": ["user1", "user2", "user3"]
            },
            {
                "request": "Mobile app", 
                "frequency": 19,
                "users": ["user4", "user5"]
            },
            {
                "request": "Advanced search filters",
                "frequency": 15,
                "users": ["user6", "user7"]
            }
        ]
        
        opportunities = []
        for request in common_requests:
            if request["frequency"] > 10:
                opportunities.append({
                    "type": "feature_request",
                    "title": f"Implement {request['request']}",
                    "description": f"Requested by {request['frequency']} users",
                    "impact_score": min(100, request["frequency"] * 3),
                    "effort_estimate": "4-6 weeks",
                    "category": "user_experience"
                })
        
        return opportunities
    
    async def create_improvement_roadmap(self) -> Dict:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ roadmap ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹"""
        opportunities = await self.analyze_improvement_opportunities()
        
        # Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ ÐºÐ²Ð°Ñ€Ñ‚Ð°Ð»Ð°Ð¼
        q1_items = opportunities[:5]  # Ð¢Ð¾Ð¿-5 Ð½Ð° Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÐºÐ²Ð°Ñ€Ñ‚Ð°Ð»
        q2_items = opportunities[5:10]
        q3_items = opportunities[10:15]
        
        roadmap = {
            "Q1_2025": {
                "theme": "Performance & Stability",
                "items": q1_items,
                "estimated_effort": "12-16 weeks",
                "expected_outcomes": [
                    "50% improvement in report generation time",
                    "99.9% system uptime",
                    "Reduced support tickets by 30%"
                ]
            },
            "Q2_2025": {
                "theme": "User Experience & Mobile",
                "items": q2_items,
                "estimated_effort": "14-18 weeks", 
                "expected_outcomes": [
                    "Mobile app launch",
                    "Improved user satisfaction to 9.2/10",
                    "Advanced search capabilities"
                ]
            },
            "Q3_2025": {
                "theme": "Advanced Analytics & AI",
                "items": q3_items,
                "estimated_effort": "16-20 weeks",
                "expected_outcomes": [
                    "Predictive analytics dashboard",
                    "AI-powered anomaly detection",
                    "Advanced business intelligence"
                ]
            }
        }
        
        return roadmap

class UserFeedbackAnalyzer:
    """ÐÐ½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹"""
    
    async def collect_feedback(self) -> List[Dict]:
        """Ð¡Ð±Ð¾Ñ€ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ Ð¸Ð· Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²"""
        # Support tickets
        support_feedback = await self.analyze_support_tickets()
        
        # User surveys
        survey_feedback = await self.analyze_user_surveys()
        
        # Usage analytics
        usage_feedback = await self.analyze_usage_patterns()
        
        return support_feedback + survey_feedback + usage_feedback
    
    async def analyze_support_tickets(self) -> List[Dict]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚Ð¸ÐºÐµÑ‚Ð¾Ð² Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸ Ð´Ð»Ñ Ð²Ñ‹ÑÐ²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²"""
        # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ - Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ‚Ð¸ÐºÐµÑ‚Ð¾Ð²
        common_issues = [
            {
                "category": "report_generation",
                "frequency": 45,
                "avg_resolution_time": "2.3 hours",
                "user_impact": "high",
                "suggested_improvement": "Add report templates and wizard"
            },
            {
                "category": "user_interface",
                "frequency": 32,
                "avg_resolution_time": "1.1 hours", 
                "user_impact": "medium",
                "suggested_improvement": "UI/UX improvements for complex forms"
            }
        ]
        
        feedback = []
        for issue in common_issues:
            if issue["frequency"] > 20:
                feedback.append({
                    "source": "support_tickets",
                    "category": issue["category"],
                    "priority": "high" if issue["user_impact"] == "high" else "medium",
                    "description": issue["suggested_improvement"],
                    "frequency": issue["frequency"]
                })
        
        return feedback
```

### 6.3 Future Enhancements Pipeline

#### Emerging Technologies Integration
```python
# future/emerging_tech.py
from dataclasses import dataclass
from typing import List, Dict
from datetime import datetime, timedelta

@dataclass
class TechInitiative:
    name: str
    description: str
    technology: str
    timeline: str
    investment_required: str
    expected_roi: str
    risk_level: str
    dependencies: List[str]

class EmergingTechRoadmap:
    """Roadmap Ð²Ð½ÐµÐ´Ñ€ÐµÐ½Ð¸Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¹"""
    
    def __init__(self):
        self.initiatives = self.create_tech_initiatives()
    
    def create_tech_initiatives(self) -> List[TechInitiative]:
        return [
            TechInitiative(
                name="Blockchain Audit Trail",
                description="Immutable blockchain-based audit trail for critical transactions",
                technology="Ethereum/Polygon",
                timeline="Q4 2025",
                investment_required="$150K - $200K",
                expected_roi="Enhanced trust, reduced audit costs by 60%",
                risk_level="Medium",
                dependencies=["Regulatory approval", "Integration with existing audit system"]
            ),
            
            TechInitiative(
                name="AI-Powered Financial Advisor",
                description="AI assistant for financial planning and cash flow optimization",
                technology="GPT-4/Claude + Custom ML Models",
                timeline="Q2 2026",
                investment_required="$300K - $400K", 
                expected_roi="20% improvement in financial decision making",
                risk_level="High",
                dependencies=["Historical data collection", "ML expertise hiring"]
            ),
            
            TechInitiative(
                name="IoT Integration for Asset Management",
                description="IoT sensors for real-time asset tracking and depreciation",
                technology="IoT sensors + Edge computing",
                timeline="Q3 2026",
                investment_required="$100K - $150K",
                expected_roi="Accurate asset valuation, reduced manual work",
                risk_level="Low",
                dependencies=["Hardware procurement", "Asset tagging"]
            ),
            
            TechInitiative(
                name="Quantum-Safe Cryptography",
                description="Future-proof cryptographic systems against quantum computing",
                technology="Post-quantum cryptographic algorithms",
                timeline="Q1 2027",
                investment_required="$80K - $120K",
                expected_roi="Long-term security assurance",
                risk_level="Low",
                dependencies=["Industry standards maturity", "Library availability"]
            ),
            
            TechInitiative(
                name="Voice-Activated Accounting",
                description="Voice interface for hands-free transaction entry",
                technology="Advanced Speech Recognition + NLP",
                timeline="Q2 2026", 
                investment_required="$120K - $180K",
                expected_roi="40% faster data entry, improved accessibility",
                risk_level="Medium",
                dependencies=["Georgian language NLP models", "Voice security protocols"]
            ),
            
            TechInitiative(
                name="Augmented Reality Financial Dashboards",
                description="AR visualization of financial data and KPIs",
                technology="AR/VR (Apple Vision Pro, Meta Quest)",
                timeline="Q4 2026",
                investment_required="$200K - $300K",
                expected_roi="Enhanced data understanding, executive engagement",
                risk_level="High",
                dependencies=["AR device adoption", "3D visualization expertise"]
            )
        ]
    
    def prioritize_initiatives(self) -> List[TechInitiative]:
        """ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ñ‚Ð¸Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ROI Ð¸ Ñ€Ð¸ÑÐºÐ°"""
        # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ scoring algorithm
        def calculate_priority_score(initiative: TechInitiative) -> float:
            roi_score = {
                "Enhanced trust, reduced audit costs by 60%": 9.0,
                "20% improvement in financial decision making": 8.5,
                "Accurate asset valuation, reduced manual work": 7.0,
                "Long-term security assurance": 6.0,
                "40% faster data entry, improved accessibility": 8.0,
                "Enhanced data understanding, executive engagement": 6.5
            }.get(initiative.expected_roi, 5.0)
            
            risk_penalty = {
                "Low": 0,
                "Medium": 1,
                "High": 2
            }.get(initiative.risk_level, 1)
            
            return roi_score - risk_penalty
        
        initiatives = self.initiatives.copy()
        initiatives.sort(key=calculate_priority_score, reverse=True)
        return initiatives
    
    def create_innovation_budget(self) -> Dict:
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð° Ð½Ð° Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¸"""
        prioritized = self.prioritize_initiatives()
        
        # Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð³Ð¾Ð´Ð°Ð¼
        budget_2025 = []
        budget_2026 = []  
        budget_2027 = []
        
        for initiative in prioritized:
            if "2025" in initiative.timeline:
                budget_2025.append(initiative)
            elif "2026" in initiative.timeline:
                budget_2026.append(initiative)
            else:
                budget_2027.append(initiative)
        
        def calculate_budget(initiatives):
            total = 0
            for init in initiatives:
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ ÑÑ€ÐµÐ´Ð½ÑŽÑŽ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¸Ð· Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð°
                cost_str = init.investment_required.replace("$", "").replace("K", "000")
                if " - " in cost_str:
                    min_cost, max_cost = cost_str.split(" - ")
                    avg_cost = (int(min_cost) + int(max_cost)) / 2
                    total += avg_cost
            return total
        
        return {
            "2025": {
                "initiatives": budget_2025,
                "total_budget": calculate_budget(budget_2025),
                "focus": "Foundation & Security"
            },
            "2026": {
                "initiatives": budget_2026,
                "total_budget": calculate_budget(budget_2026),
                "focus": "AI & User Experience"
            },
            "2027": {
                "initiatives": budget_2027, 
                "total_budget": calculate_budget(budget_2027),
                "focus": "Future-Proofing"
            }
        }

# Georgian Market Specific Innovations
class GeorgianMarketInnovations:
    """Ð˜Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¸ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð³Ð¾ Ñ€Ñ‹Ð½ÐºÐ°"""
    
    def __init__(self):
        self.georgian_initiatives = [
            {
                "name": "Georgian Language AI Assistant",
                "description": "AI Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°",
                "market_need": "Ð›Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹",
                "timeline": "Q3 2025",
                "partners": ["Tbilisi State University", "Georgian Tech Companies"]
            },
            {
                "name": "Integration with Georgian Banking APIs", 
                "description": "ÐŸÑ€ÑÐ¼Ð°Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ API Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¸Ñ… Ð±Ð°Ð½ÐºÐ¾Ð²",
                "market_need": "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ð°Ð½ÐºÐ¾Ð²ÑÐºÐ¸Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹",
                "timeline": "Q2 2025",
                "partners": ["Bank of Georgia", "TBC Bank", "Liberty Bank"]
            },
            {
                "name": "Georgian Tax Law AI Advisor",
                "description": "AI ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¹ Ð¿Ð¾ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¼Ñƒ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ð¼Ñƒ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ñƒ", 
                "market_need": "Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°",
                "timeline": "Q4 2025",
                "partners": ["Ministry of Finance", "Georgian Tax Consultants Association"]
            },
            {
                "name": "Small Business Starter Package",
                "description": "Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ð´Ð»Ñ Ð¼Ð°Ð»Ð¾Ð³Ð¾ Ð±Ð¸Ð·Ð½ÐµÑÐ° Ð´Ð¾ 100,000 Ð»Ð°Ñ€Ð¸",
                "market_need": "Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð»Ñ Ð¼Ð°Ð»Ð¾Ð³Ð¾ Ð±Ð¸Ð·Ð½ÐµÑÐ°",
                "timeline": "Q1 2025",
                "partners": ["Enterprise Georgia", "GITA"]
            }
        ]
    
    def create_market_expansion_plan(self) -> Dict:
        """ÐŸÐ»Ð°Ð½ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ñ Ð½Ð° Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¼ Ñ€Ñ‹Ð½ÐºÐµ"""
        return {
            "target_segments": {
                "small_business": {
                    "size": "5,000+ companies",
                    "revenue_potential": "$2M annually",
                    "key_features": ["Simplified interface", "Georgian support", "Low cost"]
                },
                "medium_enterprises": {
                    "size": "1,500+ companies", 
                    "revenue_potential": "$8M annually",
                    "key_features": ["Full IFRS", "Advanced reporting", "Integrations"]
                },
                "large_corporations": {
                    "size": "200+ companies",
                    "revenue_potential": "$12M annually", 
                    "key_features": ["Multi-entity", "Custom workflows", "Dedicated support"]
                }
            },
            "market_penetration_strategy": {
                "year_1": "Capture 15% of medium enterprise market",
                "year_2": "Expand to small business segment", 
                "year_3": "Target large corporations and government",
                "year_5": "Market leadership position (40%+ market share)"
            },
            "competitive_advantages": [
                "Only IFRS + Georgian Tax compliant solution",
                "Local language and support",
                "Integration with Georgian financial ecosystem", 
                "Reasonable pricing for local market"
            ]
        }
```

---

## ðŸŽ¯
                # ÐŸÐ»Ð°Ð½ Ð¿Ð¾ÑÑ‚Ð°Ð¿Ð½Ð¾Ð¹ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¾Ð¹ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€ÑÐºÐ¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹

## ðŸŽ¯ ÐžÐ±Ñ‰Ð°Ñ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸

**ÐŸÐ¾Ð´Ñ…Ð¾Ð´**: Strangler Fig Pattern - Ð¿Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ð°Ñ Ð·Ð°Ð¼ÐµÐ½Ð° ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ð±ÐµÐ· Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹

**ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ**: 18-24 Ð¼ÐµÑÑÑ†Ð°

**Ð‘ÑŽÐ´Ð¶ÐµÑ‚**: Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½ Ð¿Ð¾ÑÑ‚Ð°Ð¿Ð½Ð¾ Ð´Ð»Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ€Ð¸ÑÐºÐ¾Ð²

---

## ðŸ“‹ Ð¤Ð°Ð·Ð° 0: ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· (ÐœÐµÑÑÑ†Ñ‹ 1-2)

### Ð¦ÐµÐ»Ð¸:
- ÐÑƒÐ´Ð¸Ñ‚ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ
- ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹
- Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸

### Ð—Ð°Ð´Ð°Ñ‡Ð¸:

#### ÐÐµÐ´ÐµÐ»Ñ 1-2: Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°ÑƒÐ´Ð¸Ñ‚
```bash
# ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ¾Ð´Ð°
- Ð˜Ð½Ð²ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¹ Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
- ÐžÑ†ÐµÐ½ÐºÐ° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ñ (Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹: ~10%, Ñ†ÐµÐ»ÑŒ: 85%)
- Ð’Ñ‹ÑÐ²Ð»ÐµÐ½Ð¸Ðµ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑƒÐ·ÐºÐ¸Ñ… Ð¼ÐµÑÑ‚ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
- ÐÐ½Ð°Ð»Ð¸Ð· Ð´Ð°Ð½Ð½Ñ‹Ñ…: Ð¾Ð±ÑŠÐµÐ¼Ñ‹, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°, quality issues
```

#### ÐÐµÐ´ÐµÐ»Ñ 3-4: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ MVP Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹
```yaml
# docker-compose-migration.yml
version: '3.8'
services:
  # Ð¢ÐµÐºÑƒÑ‰Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° (legacy)
  legacy-app:
    build: ./legacy
    ports:
      - "8080:8080"
    networks:
      - migration-network
  
  # ÐÐ¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° (target)
  new-api-gateway:
    build: ./new-system/gateway
    ports:
      - "8000:8000"
    networks:
      - migration-network
  
  # Shared resources
  postgres-new:
    image: postgres:15
    environment:
      POSTGRES_DB: accounting_new
    networks:
      - migration-network
  
  redis:
    image: redis:7-alpine
    networks:
      - migration-network

networks:
  migration-network:
    driver: bridge
```

#### ÐÐµÐ´ÐµÐ»Ñ 5-6: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Data Pipeline
```python
# migration/data_sync.py
from sqlalchemy import create_engine
import asyncpg
import pandas as pd

class DataSynchronizer:
    def __init__(self):
        self.legacy_engine = create_engine('postgresql://legacy_db')
        self.new_pool = None
    
    async def sync_accounts(self):
        """Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸ÐºÐ° ÑÑ‡ÐµÑ‚Ð¾Ð²"""
        df = pd.read_sql("SELECT * FROM chart_of_accounts", self.legacy_engine)
        
        # Transform data
        df['id'] = df.apply(lambda x: uuid4(), axis=1)
        df['created_at'] = pd.Timestamp.now()
        
        # Load to new system
        async with self.new_pool.acquire() as conn:
            await conn.executemany(
                "INSERT INTO accounts (id, code, name, type) VALUES ($1, $2, $3, $4)",
                df[['id', 'code', 'name', 'account_type']].values.tolist()
            )
```

#### ÐÐµÐ´ÐµÐ»Ñ 7-8: CI/CD Pipeline
```yaml
# .github/workflows/migration.yml
name: Migration Pipeline
on:
  push:
    branches: [main, migration/*]

jobs:
  test-legacy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Test Legacy System
        run: |
          docker-compose -f legacy/docker-compose.test.yml up --abort-on-container-exit
  
  test-new-system:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Test New System
        run: |
          docker-compose -f new-system/docker-compose.test.yml up --abort-on-container-exit
  
  deploy-staging:
    needs: [test-legacy, test-new-system]
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to Staging
        run: |
          kubectl apply -f k8s/staging/
```

### Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¤Ð°Ð·Ñ‹ 0:
- âœ… Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ñ€Ð°Ð¼ÐºÐ°Ð¼Ð¸
- âœ… ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ð°Ñ Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð´Ð»Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ ÑÐ¸ÑÑ‚ÐµÐ¼
- âœ… ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ðº Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ (3-5 Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¾Ð²)
- âœ… Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð° Ð¸ Ð°Ð»ÐµÑ€Ñ‚Ð¾Ð²

---

## ðŸ”§ Ð¤Ð°Ð·Ð° 1: ÐœÐ¾Ð´ÐµÑ€Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð° (ÐœÐµÑÑÑ†Ñ‹ 3-6)

### Ð¦ÐµÐ»Ð¸:
- Ð—Ð°Ð¼ÐµÐ½Ð° ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²
- ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸
- Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸

### 1.1 ÐœÐ¾Ð´ÐµÑ€Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐœÐµÑÑÑ† 3)

#### ÐÐµÐ´ÐµÐ»Ñ 1-2: ÐÐ¾Ð²Ð°Ñ ÑÑ…ÐµÐ¼Ð° Ð‘Ð”
```sql
-- migrations/001_new_schema.sql
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";

-- ÐÐ¾Ð²Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° ÑÑ‡ÐµÑ‚Ð¾Ð² Ñ UUID
CREATE TABLE accounts_v2 (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    code VARCHAR(20) NOT NULL,
    name VARCHAR(100) NOT NULL,
    account_type account_type_enum NOT NULL,
    parent_id UUID REFERENCES accounts_v2(id),
    is_active BOOLEAN DEFAULT TRUE,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    version INTEGER DEFAULT 1,
    
    -- Ð˜Ð½Ð´ÐµÐºÑÑ‹ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
    CONSTRAINT unique_code_per_company UNIQUE (code, company_id)
);

CREATE INDEX idx_accounts_v2_code ON accounts_v2(code);
CREATE INDEX idx_accounts_v2_type ON accounts_v2(account_type);
CREATE INDEX idx_accounts_v2_parent ON accounts_v2(parent_id);
```

#### ÐÐµÐ´ÐµÐ»Ñ 3-4: ÐŸÐ°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†
```sql
-- ÐŸÐ°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð¾Ðº Ð¿Ð¾ Ð¼ÐµÑÑÑ†Ð°Ð¼
CREATE TABLE journal_entries_v2 (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    transaction_id UUID NOT NULL,
    account_id UUID NOT NULL REFERENCES accounts_v2(id),
    debit DECIMAL(15,2) DEFAULT 0.00,
    credit DECIMAL(15,2) DEFAULT 0.00,
    currency_code CHAR(3) NOT NULL DEFAULT 'GEL',
    exchange_rate DECIMAL(15,5) DEFAULT 1.00000,
    description TEXT,
    transaction_date DATE NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT check_debit_credit CHECK (
        (debit > 0 AND credit = 0) OR (credit > 0 AND debit = 0)
    )
) PARTITION BY RANGE (transaction_date);

-- Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¹ Ð½Ð° Ð³Ð¾Ð´ Ð²Ð¿ÐµÑ€ÐµÐ´
CREATE TABLE journal_entries_202501 PARTITION OF journal_entries_v2
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
-- ... Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑÑÑ†Ñ‹
```

### 1.2 ÐÐ¾Ð²Ñ‹Ð¹ API ÑÐ»Ð¾Ð¹ (ÐœÐµÑÑÑ† 4)

#### FastAPI Ñ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð°Ð¼Ð¸
```python
# new_system/api/main.py
from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from fastapi.security import HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import structlog

logger = structlog.get_logger()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_database_pool()
    await init_kafka_producer()
    logger.info("Application started")
    
    yield
    
    # Shutdown
    await close_database_pool()
    await close_kafka_producer()
    logger.info("Application stopped")

app = FastAPI(
    title="Georgian Accounting System v2.0",
    description="Modern IFRS-compliant accounting system",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure properly in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API Routes
from .routes import accounts, transactions, reports

app.include_router(accounts.router, prefix="/api/v1/accounts")
app.include_router(transactions.router, prefix="/api/v1/transactions")
app.include_router(reports.router, prefix="/api/v1/reports")
```

#### Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ñ DI
```python
# new_system/core/dependencies.py
from dependency_injector import containers, providers
from dependency_injector.wiring import Provide

class Container(containers.DeclarativeContainer):
    # Configuration
    config = providers.Configuration()
    
    # Database
    db_pool = providers.Singleton(
        create_async_pool,
        config.database.url
    )
    
    # Repositories
    account_repository = providers.Factory(
        AccountRepository,
        db_pool=db_pool
    )
    
    transaction_repository = providers.Factory(
        TransactionRepository,
        db_pool=db_pool
    )
    
    # Services
    accounting_service = providers.Factory(
        AccountingService,
        account_repo=account_repository,
        transaction_repo=transaction_repository
    )

# Dependency injection
async def get_accounting_service(
    service: AccountingService = Depends(Provide[Container.accounting_service])
) -> AccountingService:
    return service
```

### 1.3 Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ (ÐœÐµÑÑÑ† 5)

#### JWT Ñ refresh tokens
```python
# new_system/auth/jwt_handler.py
from jose import JWTError, jwt
from datetime import datetime, timedelta
import secrets

class JWTHandler:
    def __init__(self, secret_key: str):
        self.secret_key = secret_key
        self.algorithm = "HS256"
        self.access_token_expire = timedelta(minutes=30)
        self.refresh_token_expire = timedelta(days=7)
    
    async def create_tokens(self, user_id: str, permissions: List[str]) -> TokenPair:
        access_payload = {
            "sub": user_id,
            "permissions": permissions,
            "type": "access",
            "exp": datetime.utcnow() + self.access_token_expire,
            "iat": datetime.utcnow(),
            "jti": secrets.token_hex(16)  # JWT ID Ð´Ð»Ñ Ð¾Ñ‚Ð·Ñ‹Ð²Ð°
        }
        
        refresh_payload = {
            "sub": user_id,
            "type": "refresh",
            "exp": datetime.utcnow() + self.refresh_token_expire,
            "iat": datetime.utcnow(),
            "jti": secrets.token_hex(16)
        }
        
        access_token = jwt.encode(access_payload, self.secret_key, self.algorithm)
        refresh_token = jwt.encode(refresh_payload, self.secret_key, self.algorithm)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ refresh token Ð² Redis Ñ TTL
        await self.redis.setex(f"refresh:{refresh_payload['jti']}", 
                              int(self.refresh_token_expire.total_seconds()),
                              user_id)
        
        return TokenPair(
            access_token=access_token,
            refresh_token=refresh_token,
            expires_in=int(self.access_token_expire.total_seconds())
        )
```

#### Role-Based Access Control
```python
# new_system/auth/rbac.py
from enum import Enum
from dataclasses import dataclass
from typing import Set

class Permission(Enum):
    ACCOUNTS_READ = "accounts:read"
    ACCOUNTS_WRITE = "accounts:write"
    TRANSACTIONS_READ = "transactions:read"
    TRANSACTIONS_WRITE = "transactions:write"
    TRANSACTIONS_APPROVE = "transactions:approve"
    REPORTS_FINANCIAL = "reports:financial"
    REPORTS_TAX = "reports:tax"
    ADMIN_USERS = "admin:users"
    ADMIN_SYSTEM = "admin:system"

@dataclass
class Role:
    name: str
    permissions: Set[Permission]

class GeorgianAccountingRoles:
    ACCOUNTANT = Role("accountant", {
        Permission.ACCOUNTS_READ,
        Permission.TRANSACTIONS_READ,
        Permission.TRANSACTIONS_WRITE,
        Permission.REPORTS_FINANCIAL
    })
    
    CHIEF_ACCOUNTANT = Role("chief_accountant", {
        *ACCOUNTANT.permissions,
        Permission.TRANSACTIONS_APPROVE,
        Permission.REPORTS_TAX,
        Permission.ADMIN_USERS
    })
    
    TAX_SPECIALIST = Role("tax_specialist", {
        Permission.ACCOUNTS_READ,
        Permission.TRANSACTIONS_READ,
        Permission.REPORTS_TAX
    })

def require_permission(permission: Permission):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, current_user = Depends(get_current_user), **kwargs):
            if permission not in current_user.permissions:
                raise HTTPException(403, "Insufficient permissions")
            return await func(*args, **kwargs, current_user=current_user)
        return wrapper
    return decorator
```

### 1.4 ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ (ÐœÐµÑÑÑ† 6)

#### Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
```python
# new_system/core/logging.py
import structlog
from pythonjsonlogger import jsonlogger

def setup_logging():
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² ÐºÐ¾Ð´Ðµ
logger = structlog.get_logger()

async def create_transaction(transaction_data: TransactionCreate):
    logger.info(
        "Transaction creation started",
        transaction_id=transaction_data.id,
        user_id=current_user.id,
        amount=float(transaction_data.total_amount)
    )
    
    try:
        result = await service.create_transaction(transaction_data)
        logger.info(
            "Transaction created successfully",
            transaction_id=result.id,
            duration_ms=(time.time() - start_time) * 1000
        )
        return result
    except Exception as e:
        logger.error(
            "Transaction creation failed",
            error=str(e),
            transaction_data=transaction_data.dict()
        )
        raise
```

#### Prometheus Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
```python
# new_system/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time

# Business metrics
transaction_counter = Counter(
    'accounting_transactions_total',
    'Total number of accounting transactions',
    ['status', 'transaction_type']
)

transaction_amount_histogram = Histogram(
    'accounting_transaction_amount_gel',
    'Distribution of transaction amounts in GEL',
    buckets=[10, 50, 100, 500, 1000, 5000, 10000, 50000, float('inf')]
)

account_balance_gauge = Gauge(
    'accounting_account_balance_gel',
    'Current account balance in GEL',
    ['account_code', 'account_type']
)

# Technical metrics
request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint', 'status']
)

class MetricsMiddleware:
    async def __call__(self, request, call_next):
        start_time = time.time()
        response = await call_next(request)
        duration = time.time() - start_time
        
        request_duration.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).observe(duration)
        
        return response
```

### Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¤Ð°Ð·Ñ‹ 1:
- âœ… ÐÐ¾Ð²Ð°Ñ Ð‘Ð” ÑÑ…ÐµÐ¼Ð° Ñ UUID Ð¸ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼
- âœ… Modern FastAPI Ñ async/await
- âœ… JWT authentication Ñ RBAC
- âœ… Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
- âœ… 60% Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð°Ð¼Ð¸ Ð½Ð¾Ð²Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²

---

## ðŸ—ï¸ Ð¤Ð°Ð·Ð° 2: Ð’Ð½ÐµÐ´Ñ€ÐµÐ½Ð¸Ðµ Event-Driven Architecture (ÐœÐµÑÑÑ†Ñ‹ 7-10)

### Ð¦ÐµÐ»Ð¸:
- Ð ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Event Sourcing Ð´Ð»Ñ audit trail
- CQRS Ð´Ð»Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ/Ð·Ð°Ð¿Ð¸ÑÐ¸
- Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ Kafka

### 2.1 Event Store Ð¸ Event Sourcing (ÐœÐµÑÑÑ† 7)

#### Event Store Ð½Ð° PostgreSQL
```sql
-- events/001_event_store.sql
CREATE TABLE event_store (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    aggregate_id UUID NOT NULL,
    aggregate_type VARCHAR(100) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    event_data JSONB NOT NULL,
    event_metadata JSONB DEFAULT '{}',
    version INTEGER NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT unique_version_per_aggregate UNIQUE (aggregate_id, version)
);

CREATE INDEX idx_event_store_aggregate ON event_store(aggregate_id);
CREATE INDEX idx_event_store_type ON event_store(event_type);
CREATE INDEX idx_event_store_timestamp ON event_store(timestamp);

-- Snapshots Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
CREATE TABLE aggregate_snapshots (
    aggregate_id UUID PRIMARY KEY,
    aggregate_type VARCHAR(100) NOT NULL,
    snapshot_data JSONB NOT NULL,
    version INTEGER NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

#### Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ ÐºÐ»Ð°ÑÑÑ‹ Ð´Ð»Ñ Event Sourcing
```python
# new_system/events/base.py
from dataclasses import dataclass
from typing import Any, List, Dict
from abc import ABC, abstractmethod
import uuid
from datetime import datetime

@dataclass(frozen=True)
class DomainEvent:
    """Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ Ð´Ð¾Ð¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹"""
    aggregate_id: uuid.UUID
    event_id: uuid.UUID
    event_type: str
    event_data: Dict[str, Any]
    version: int
    timestamp: datetime
    metadata: Dict[str, Any]

class Aggregate(ABC):
    """Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ð²"""
    def __init__(self, aggregate_id: uuid.UUID):
        self.id = aggregate_id
        self.version = 0
        self.uncommitted_events: List[DomainEvent] = []
    
    def apply_event(self, event: DomainEvent):
        """ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ðº Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ñƒ"""
        self._apply_event(event)
        if event.version > self.version:
            self.version = event.version
    
    def raise_event(self, event_type: str, event_data: Dict[str, Any], metadata: Dict[str, Any] = None):
        """ÐŸÐ¾Ð´Ð½ÑÑ‚ÑŒ Ð½Ð¾Ð²Ð¾Ðµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ"""
        event = DomainEvent(
            aggregate_id=self.id,
            event_id=uuid.uuid4(),
            event_type=event_type,
            event_data=event_data,
            version=self.version + 1,
            timestamp=datetime.utcnow(),
            metadata=metadata or {}
        )
        self.uncommitted_events.append(event)
        self.apply_event(event)
    
    @abstractmethod
    def _apply_event(self, event: DomainEvent):
        """ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ðº ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð°"""
        pass
    
    def mark_events_as_committed(self):
        """ÐŸÐ¾Ð¼ÐµÑ‚Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ ÐºÐ°Ðº ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ"""
        self.uncommitted_events.clear()
```

#### Accounting Aggregate
```python
# new_system/domain/aggregates.py
from decimal import Decimal
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class JournalEntryData:
    account_id: uuid.UUID
    debit: Decimal
    credit: Decimal
    description: str

class AccountingTransaction(Aggregate):
    """ÐÐ³Ñ€ÐµÐ³Ð°Ñ‚ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€ÑÐºÐ¾Ð¹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
    
    def __init__(self, aggregate_id: uuid.UUID):
        super().__init__(aggregate_id)
        self.transaction_date: Optional[datetime] = None
        self.description: str = ""
        self.entries: List[JournalEntryData] = []
        self.status: str = "draft"
        self.total_debit: Decimal = Decimal('0.00')
        self.total_credit: Decimal = Decimal('0.00')
    
    def create_transaction(self, transaction_date: datetime, description: str, entries: List[JournalEntryData]):
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ð²ÑƒÑŽ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ"""
        if self.status != "":
            raise ValueError("Transaction already exists")
        
        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð²Ð¾Ð¹Ð½Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸
        total_debit = sum(entry.debit for entry in entries)
        total_credit = sum(entry.credit for entry in entries)
        
        if total_debit != total_credit:
            raise ValueError(f"Unbalanced transaction: debit={total_debit}, credit={total_credit}")
        
        self.raise_event("TransactionCreated", {
            "transaction_date": transaction_date.isoformat(),
            "description": description,
            "entries": [
                {
                    "account_id": str(entry.account_id),
                    "debit": str(entry.debit),
                    "credit": str(entry.credit),
                    "description": entry.description
                }
                for entry in entries
            ],
            "total_amount": str(total_debit)
        })
    
    def approve_transaction(self, approved_by: uuid.UUID):
        """ÐžÐ´Ð¾Ð±Ñ€Ð¸Ñ‚ÑŒ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ"""
        if self.status != "draft":
            raise ValueError(f"Cannot approve transaction with status: {self.status}")
        
        self.raise_event("TransactionApproved", {
            "approved_by": str(approved_by),
            "approved_at": datetime.utcnow().isoformat()
        })
    
    def post_transaction(self, posted_by: uuid.UUID):
        """ÐŸÑ€Ð¾Ð²ÐµÑÑ‚Ð¸ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ"""
        if self.status != "approved":
            raise ValueError(f"Cannot post transaction with status: {self.status}")
        
        self.raise_event("TransactionPosted", {
            "posted_by": str(posted_by),
            "posted_at": datetime.utcnow().isoformat()
        })
    
    def _apply_event(self, event: DomainEvent):
        """ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ðº ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
        if event.event_type == "TransactionCreated":
            self.transaction_date = datetime.fromisoformat(event.event_data["transaction_date"])
            self.description = event.event_data["description"]
            self.entries = [
                JournalEntryData(
                    account_id=uuid.UUID(entry["account_id"]),
                    debit=Decimal(entry["debit"]),
                    credit=Decimal(entry["credit"]),
                    description=entry["description"]
                )
                for entry in event.event_data["entries"]
            ]
            self.total_debit = self.total_credit = Decimal(event.event_data["total_amount"])
            self.status = "draft"
            
        elif event.event_type == "TransactionApproved":
            self.status = "approved"
            
        elif event.event_type == "TransactionPosted":
            self.status = "posted"
```

### 2.2 CQRS Implementation (ÐœÐµÑÑÑ† 8)

#### Command Ð¸ Query Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ
```python
# new_system/cqrs/commands.py
from dataclasses import dataclass
from abc import ABC, abstractmethod

class Command(ABC):
    """Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´"""
    pass

class CommandHandler(ABC):
    @abstractmethod
    async def handle(self, command: Command) -> Any:
        pass

@dataclass
class CreateTransactionCommand(Command):
    transaction_date: datetime
    description: str
    entries: List[JournalEntryData]
    created_by: uuid.UUID

class CreateTransactionHandler(CommandHandler):
    def __init__(self, event_store: EventStore, event_bus: EventBus):
        self.event_store = event_store
        self.event_bus = event_bus
    
    async def handle(self, command: CreateTransactionCommand) -> uuid.UUID:
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚
        transaction_id = uuid.uuid4()
        transaction = AccountingTransaction(transaction_id)
        
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð±Ð¸Ð·Ð½ÐµÑ-Ð»Ð¾Ð³Ð¸ÐºÑƒ
        transaction.create_transaction(
            command.transaction_date,
            command.description,
            command.entries
        )
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ
        await self.event_store.save_events(
            transaction.id,
            transaction.uncommitted_events,
            expected_version=0
        )
        
        # ÐžÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ¾Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ
        for event in transaction.uncommitted_events:
            await self.event_bus.publish(event)
        
        transaction.mark_events_as_committed()
        return transaction.id
```

#### Query side (Read Models)
```python
# new_system/cqrs/queries.py
@dataclass
class AccountBalanceQuery:
    account_id: uuid.UUID
    as_of_date: Optional[datetime] = None

class AccountBalanceQueryHandler:
    def __init__(self, read_db_pool):
        self.read_db = read_db_pool
    
    async def handle(self, query: AccountBalanceQuery) -> Decimal:
        async with self.read_db.acquire() as conn:
            if query.as_of_date:
                result = await conn.fetchval(
                    """
                    SELECT balance FROM account_balances_history 
                    WHERE account_id = $1 AND date <= $2 
                    ORDER BY date DESC LIMIT 1
                    """,
                    query.account_id,
                    query.as_of_date
                )
            else:
                result = await conn.fetchval(
                    "SELECT current_balance FROM account_balances WHERE account_id = $1",
                    query.account_id
                )
            
            return Decimal(str(result or '0.00'))

# Projection Ð´Ð»Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ñ read models
class AccountBalanceProjection:
    def __init__(self, read_db_pool):
        self.read_db = read_db_pool
    
    async def handle_transaction_posted(self, event: DomainEvent):
        """ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð±Ð°Ð»Ð°Ð½ÑÑ‹ ÑÑ‡ÐµÑ‚Ð¾Ð² Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ð¸ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
        entries = event.event_data["entries"]
        
        async with self.read_db.acquire() as conn:
            async with conn.transaction():
                for entry_data in entries:
                    account_id = uuid.UUID(entry_data["account_id"])
                    debit = Decimal(entry_data["debit"])
                    credit = Decimal(entry_data["credit"])
                    
                    # ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð±Ð°Ð»Ð°Ð½Ñ
                    await conn.execute(
                        """
                        INSERT INTO account_balances (account_id, current_balance, last_updated)
                        VALUES ($1, $2, $3)
                        ON CONFLICT (account_id) DO UPDATE SET
                            current_balance = account_balances.current_balance + $2,
                            last_updated = $3
                        """,
                        account_id,
                        debit - credit,
                        event.timestamp
                    )
                    
                    # Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð·Ð°Ð¿Ð¸ÑÑŒ
                    await conn.execute(
                        """
                        INSERT INTO account_balance_history 
                        (account_id, date, balance_change, running_balance, transaction_id)
                        VALUES ($1, $2, $3, 
                            (SELECT current_balance FROM account_balances WHERE account_id = $1),
                            $4)
                        """,
                        account_id,
                        event.timestamp.date(),
                        debit - credit,
                        event.aggregate_id
                    )
```

### 2.3 Kafka Integration (ÐœÐµÑÑÑ† 9)

#### Event Bus Ñ Kafka
```python
# new_system/infrastructure/event_bus.py
from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
import json
from typing import Dict, Callable

class KafkaEventBus:
    def __init__(self, bootstrap_servers: str):
        self.bootstrap_servers = bootstrap_servers
        self.producer: Optional[AIOKafkaProducer] = None
        self.consumer = AIOKafkaConsumer(
            *topics,
            bootstrap_servers=self.bootstrap_servers,
            group_id="accounting-system",
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        await self.consumer.start()
        
        try:
            async for msg in self.consumer:
                await self._handle_message(msg)
        finally:
            await self.consumer.stop()
    
    async def _handle_message(self, msg):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð²Ñ…Ð¾Ð´ÑÑ‰ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ"""
        try:
            event_data = msg.value
            event_type = event_data["event_type"]
            
            if event_type in self.handlers:
                for handler in self.handlers[event_type]:
                    await handler(event_data)
                    
        except Exception as e:
            logger.error(
                "Event handling failed",
                error=str(e),
                topic=msg.topic,
                partition=msg.partition,
                offset=msg.offset
            )
```

### 2.4 Georgian Tax Service Integration (ÐœÐµÑÑÑ† 10)

#### Real-time VAT reporting
```python
# new_system/integrations/georgian_tax.py
class GeorgianTaxEventHandler:
    def __init__(self, rs_client: RSApiClient):
        self.rs_client = rs_client
    
    async def handle_transaction_posted(self, event_data: Dict):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð½ÑƒÑŽ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ Ð´Ð»Ñ Ð½Ð°Ð»Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ ÑƒÑ‡ÐµÑ‚Ð°"""
        transaction_id = event_data["aggregate_id"]
        entries = event_data["event_data"]["entries"]
        
        # ÐÐ°Ð¹Ñ‚Ð¸ VAT-related Ð¿Ñ€Ð¾Ð²Ð¾Ð´ÐºÐ¸
        vat_entries = []
        for entry in entries:
            account = await self.get_account_info(entry["account_id"])
            if account.account_type == "VAT_PAYABLE" or account.account_type == "VAT_RECEIVABLE":
                vat_entries.append({
                    "account_id": entry["account_id"],
                    "amount": entry["credit"] if entry["credit"] > 0 else entry["debit"],
                    "type": "payable" if account.account_type == "VAT_PAYABLE" else "receivable"
                })
        
        if vat_entries:
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð² Georgian Revenue Service
            await self._notify_rs_about_vat_transaction(transaction_id, vat_entries)
    
    async def _notify_rs_about_vat_transaction(self, transaction_id: str, vat_entries: List[Dict]):
        """Ð£Ð²ÐµÐ´Ð¾Ð¼Ð¸Ñ‚ÑŒ RS.ge Ð¾ VAT Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
        try:
            payload = {
                "transaction_id": transaction_id,
                "timestamp": datetime.utcnow().isoformat(),
                "vat_entries": vat_entries,
                "company_id": self.company_id
            }
            
            response = await self.rs_client.post("/api/v1/vat/transactions", payload)
            
            logger.info(
                "VAT transaction reported to RS.ge",
                transaction_id=transaction_id,
                rs_response_status=response.status
            )
            
        except Exception as e:
            logger.error(
                "Failed to report VAT transaction to RS.ge",
                transaction_id=transaction_id,
                error=str(e)
            )
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð² Dead Letter Queue Ð´Ð»Ñ retry
            await self.send_to_dlq("vat_reporting", payload)
```

### Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¤Ð°Ð·Ñ‹ 2:
- âœ… Event Store Ñ Ð¿Ð¾Ð»Ð½Ñ‹Ð¼ audit trail
- âœ… CQRS Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ read/write Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
- âœ… Kafka Ð´Ð»Ñ event streaming
- âœ… Real-time Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ Georgian Tax Service
- âœ… 80% Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð°Ð¼Ð¸ event-driven ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²

---

## ðŸŽ¯ Ð¤Ð°Ð·Ð° 3: ÐœÐ¸ÐºÑ€Ð¾ÑÐµÑ€Ð²Ð¸ÑÐ½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° (ÐœÐµÑÑÑ†Ñ‹ 11-14)

### Ð¦ÐµÐ»Ð¸:
- Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹
- API Gateway
- Service Mesh
- Container orchestration

### 3.1 Ð”ÐµÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ñ Ð½Ð° Ð¼Ð¸ÐºÑ€Ð¾ÑÐµÑ€Ð²Ð¸ÑÑ‹ (ÐœÐµÑÑÑ† 11)

#### Domain-Driven Design Ð¿Ð¾Ð´Ñ…Ð¾Ð´
```
Bounded Contexts:
â”œâ”€â”€ ðŸ“ accounting-core-service/        # ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð±ÑƒÑ…Ð³Ð°Ð»Ñ‚ÐµÑ€ÑÐºÐ¸Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
â”œâ”€â”€ ðŸ“ tax-service/                    # ÐÐ°Ð»Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ ÑƒÑ‡ÐµÑ‚ Ð¸ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð½Ð¾ÑÑ‚ÑŒ
â”œâ”€â”€ ðŸ“ payroll-service/                # Ð—Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ð° Ð¸ ÐºÐ°Ð´Ñ€Ñ‹
â”œâ”€â”€ ðŸ“ inventory-service/              # Ð¡ÐºÐ»Ð°Ð´ÑÐºÐ¾Ð¹ ÑƒÑ‡ÐµÑ‚
â”œâ”€â”€ ðŸ“ reporting-service/              # Ð¤Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð°Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð½Ð¾ÑÑ‚ÑŒ
â”œâ”€â”€ ðŸ“ compliance-service/             # Ð¡Ð¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼
â”œâ”€â”€ ðŸ“ integration-service/            # Ð’Ð½ÐµÑˆÐ½Ð¸Ðµ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸
â”œâ”€â”€ ðŸ“ notification-service/           # Ð£Ð²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ
â””â”€â”€ ðŸ“ audit-service/                  # ÐÑƒÐ´Ð¸Ñ‚ Ð¸ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
```

#### Accounting Core Service
```python
# services/accounting-core/main.py
from fastapi import FastAPI
from .api import transactions, accounts, fiscal_periods
from .domain import AccountingDomain
from .infrastructure import EventStore, MessageBus

class AccountingCoreService:
    def __init__(self):
        self.app = FastAPI(
            title="Accounting Core Service",
            description="Core accounting operations and journal entries",
            version="1.0.0"
        )
        
        # Domain layer
        self.domain = AccountingDomain()
        
        # Infrastructure
        self.event_store = EventStore()
        self.message_bus = MessageBus()
        
        # API routes
        self.app.include_router(transactions.router, prefix="/transactions")
        self.app.include_router(accounts.router, prefix="/accounts")
        self.app.include_router(fiscal_periods.router, prefix="/fiscal-periods")
        
        # Health check
        @self.app.get("/health")
        async def health_check():
            return {
                "status": "healthy",
                "service": "accounting-core",
                "version": "1.0.0",
                "dependencies": {
                    "database": await self.check_database(),
                    "event_store": await self.check_event_store(),
                    "message_bus": await self.check_message_bus()
                }
            }
    
    async def check_database(self) -> str:
        try:
            await self.domain.repository.health_check()
            return "healthy"
        except:
            return "unhealthy"

if __name__ == "__main__":
    import uvicorn
    service = AccountingCoreService()
    uvicorn.run(service.app, host="0.0.0.0", port=8001)
```

#### Tax Service (Georgian-specific)
```python
# services/tax-service/domain/georgian_tax.py
from decimal import Decimal
from datetime import date, datetime
from typing import List, Dict

class GeorgianVATCalculator:
    STANDARD_RATE = Decimal('0.18')  # 18% VAT
    REGISTRATION_THRESHOLD = Decimal('100000.00')  # 100,000 GEL
    
    def __init__(self):
        self.current_month_turnover = Decimal('0.00')
        self.annual_turnover = Decimal('0.00')
    
    def calculate_vat(self, net_amount: Decimal, is_exempt: bool = False) -> Dict[str, Decimal]:
        """Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ ÐÐ”Ð¡ Ð¿Ð¾ Ð³Ñ€ÑƒÐ·Ð¸Ð½ÑÐºÐ¸Ð¼ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°Ð¼"""
        if is_exempt:
            return {
                "net_amount": net_amount,
                "vat_amount": Decimal('0.00'),
                "gross_amount": net_amount,
                "vat_rate": Decimal('0.00')
            }
        
        vat_amount = net_amount * self.STANDARD_RATE
        gross_amount = net_amount + vat_amount
        
        return {
            "net_amount": net_amount,
            "vat_amount": vat_amount,
            "gross_amount": gross_amount,
            "vat_rate": self.STANDARD_RATE
        }
    
    def check_vat_registration_requirement(self, monthly_turnover: Decimal) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð»Ð°Ñ‚ÐµÐ»ÑŒÑ‰Ð¸ÐºÐ° ÐÐ”Ð¡"""
        return monthly_turnover >= self.REGISTRATION_THRESHOLD

class GeorgianTaxDeclarationGenerator:
    def __init__(self):
        self.rs_integration = RSIntegrationService()
    
    async def generate_monthly_vat_declaration(self, company_id: str, year: int, month: int) -> VATDeclaration:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¼ÐµÑÑÑ‡Ð½ÑƒÑŽ Ð´ÐµÐºÐ»Ð°Ñ€Ð°Ñ†Ð¸ÑŽ ÐÐ”Ð¡"""
        # Ð¡Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° Ð¼ÐµÑÑÑ†
        transactions = await self.get_vat_transactions(company_id, year, month)
        
        total_vat_payable = sum(t.vat_amount for t in transactions if t.type == "sale")
        total_vat_deductible = sum(t.vat_amount for t in transactions if t.type == "purchase")
        net_vat = total_vat_payable - total_vat_deductible
        
        declaration = VATDeclaration(
            company_id=company_id,
            period=f"{year}-{month:02d}",
            total_sales=sum(t.net_amount for t in transactions if t.type == "sale"),
            total_vat_payable=total_vat_payable,
            total_purchases=sum(t.net_amount for t in transactions if t.type == "purchase"),
            total_vat_deductible=total_vat_deductible,
            net_vat_payment=net_vat,
            due_date=date(year, month + 1 if month < 12 else year + 1, 15)
        )
        
        return declaration
    
    async def submit_to_rs_ge(self, declaration: VATDeclaration) -> RSSubmissionResult:
        """ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð´ÐµÐºÐ»Ð°Ñ€Ð°Ñ†Ð¸ÑŽ Ð² rs.ge"""
        try:
            response = await self.rs_integration.submit_vat_declaration(declaration)
            return RSSubmissionResult(
                success=True,
                submission_id=response.submission_id,
                receipt_number=response.receipt_number
            )
        except Exception as e:
            logger.error("Failed to submit VAT declaration to RS.ge", error=str(e))
            return RSSubmissionResult(
                success=False,
                error_message=str(e)
            )
```

### 3.2 API Gateway (ÐœÐµÑÑÑ† 12)

#### Kong API Gateway configuration
```yaml
# kong/kong.yml
_format_version: "3.0"

services:
  - name: accounting-core
    url: http://accounting-core-service:8001
    plugins:
      - name: rate-limiting
        config:
          minute: 1000
          hour: 10000
      - name: jwt
        config:
          secret_is_base64: false
          key_claim_name: kid
      - name: prometheus
        config:
          per_consumer: true
    
  - name: tax-service
    url: http://tax-service:8002
    plugins:
      - name: rate-limiting
        config:
          minute: 500
          hour: 5000
      - name: jwt
      - name: request-size-limiting
        config:
          allowed_payload_size: 10

routes:
  - name: accounting-transactions
    service: accounting-core
    paths:
      - /api/v1/transactions
    methods:
      - GET
      - POST
      - PUT
    plugins:
      - name: cors
        config:
          origins: ["*"]
          methods: ["GET", "POST", "PUT", "DELETE"]
  
  - name: georgian-tax
    service: tax-service
    paths:
      - /api/v1/tax
    plugins:
      - name: request-transformer
        config:
          add:
            headers:
              - "X-Georgian-Tax: true"

consumers:
  - username: accounting-system
    custom_id: accounting-system-001
    jwt_secrets:
      - algorithm: HS256
        key: accounting-jwt-key
        secret: ${JWT_SECRET}

plugins:
  - name: prometheus
    config:
      per_consumer: true
      status_code_metrics: true
      latency_metrics: true
      bandwidth_metrics: true
```

#### API Gateway Ñ authentication
```python
# api-gateway/main.py
from fastapi import FastAPI, Request, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
import httpx
import jwt
from typing import Dict

class APIGateway:
    def __init__(self):
        self.app = FastAPI(
            title="Georgian Accounting API Gateway",
            description="Central API gateway for microservices",
            version="1.0.0"
        )
        
        self.service_registry = {
            "accounting": "http://accounting-core-service:8001",
            "tax": "http://tax-service:8002",
            "payroll": "http://payroll-service:8003",
            "reporting": "http://reporting-service:8004"
        }
        
        self.setup_middleware()
        self.setup_routes()
    
    def setup_middleware(self):
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        @self.app.middleware("http")
        async def add_security_headers(request: Request, call_next):
            response = await call_next(request)
            response.headers["X-Content-Type-Options"] = "nosniff"
            response.headers["X-Frame-Options"] = "DENY"
            response.headers["X-XSS-Protection"] = "1; mode=block"
            return response
    
    def setup_routes(self):
        @self.app.api_route("/api/v1/{service_name}/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"])
        async def proxy_request(
            service_name: str,
            path: str,
            request: Request,
            current_user = Depends(self.get_current_user)
        ):
            if service_name not in self.service_registry:
                raise HTTPException(status_code=404, detail="Service not found")
            
            service_url = self.service_registry[service_name]
            target_url = f"{service_url}/{path}"
            
            # Forward request
            async with httpx.AsyncClient() as client:
                response = await client.request(
                    method=request.method,
                    url=target_url,
                    content=await request.body(),
                    headers={
                        **dict(request.headers),
                        "X-User-ID": str(current_user.id),
                        "X-User-Permissions": ",".join(current_user.permissions)
                    },
                    params=request.query_params,
                    timeout=30.0
                )
            
            return Response(
                content=response.content,
                status_code=response.status_code,
                headers=dict(response.headers)
            )
    
    async def get_current_user(self, request: Request):
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸Ð· JWT Ñ‚Ð¾ÐºÐµÐ½Ð°"""
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            raise HTTPException(401, "Missing or invalid authorization header")
        
        token = auth_header.split(" ")[1]
        try:
            payload = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
            return User(
                id=payload["sub"],
                permissions=payload.get("permissions", [])
            )
        except jwt.InvalidTokenError:
            raise HTTPException(401, "Invalid token")
```

### 3.3 Service Mesh Ñ Istio (ÐœÐµÑÑÑ† 13)

#### Istio configuration
```yaml
# istio/virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: accounting-system
spec:
  http:
  - match:
    - uri:
        prefix: /api/v1/transactions
    route:
    - destination:
        host: accounting-core-service
        port:
          number: 8001
    retries:
      attempts: 3
      perTryTimeout: 10s
    timeout: 30s
    
  - match:
    - uri:
        prefix: /api/v1/tax
    route:
    - destination:
        host: tax-service
        port:
          number: 8002
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 2s

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: accounting-services
spec:
  host: "*.accounting-system.svc.cluster.local"
  trafficPolicy:
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
```

#### Circuit Breaker implementation
```python
# shared/circuit_breaker.py
import asyncio
from enum import Enum
from datetime import datetime, timedelta
import logging

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    def __init__(self, 
                 failure_threshold: int = 5,
                 timeout: int = 60,
                 success_threshold: int = 2):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.success_threshold = success_threshold
        
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    async def call(self, func, *args, **kwargs):
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ñ‡ÐµÑ€ÐµÐ· circuit breaker"""
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                logging.info("Circuit breaker: Attempting reset")
            else:
                raise CircuitBreakerOpenError("Circuit breaker is open")
        
        try:
            result = await func(*args, **kwargs)
            self._record_success()
            return result
        except Exception as e:
            self._record_failure()
            raise e
    
    def _record_success(self):
        """Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð²"""
        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.success_threshold:
                self.state = CircuitState.CLOSED
                self.failure_count = 0
                self.success_count = 0
                logging.info("Circuit breaker: Reset to CLOSED")
    
    def _record_failure(self):
        """Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð²"""
        self.failure_count += 1
        self.last_failure_time = datetime.utcnow()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            logging.warning("Circuit breaker: Opened due to failures")
    
    def _should_attempt_reset(self) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ, ÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð»Ð¸ Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ ÑÐ±Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ circuit breaker"""
        if self.last_failure_time is None:
            return False
        
        return datetime.utcnow() - self.last_failure_time >= timedelta(seconds=self.timeout)

# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² ÑÐµÑ€Ð²Ð¸ÑÐ°Ñ…
class TaxServiceClient:
    def __init__(self):
        self.circuit_breaker = CircuitBreaker()
        self.base_url = "http://tax-service:8002"
    
    async def calculate_vat(self, amount: Decimal) -> VATCalculation:
        return await self.circuit_breaker.call(self._calculate_vat_impl, amount)
    
    async def _calculate_vat_impl(self, amount: Decimal) -> VATCalculation:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/api/v1/vat/calculate",
                json={"amount": str(amount)}
            )
            response.raise_for_status()
            return VATCalculation(**response.json())
```

### 3.4 Container Orchestration Ñ Kubernetes (ÐœÐµÑÑÑ† 14)

#### Kubernetes deployment manifests
```yaml
# k8s/accounting-core-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: accounting-core-service
  namespace: accounting-system
  labels:
    app: accounting-core
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: accounting-core
      version: v1
  template:
    metadata:
      labels:
        app: accounting-core
        version: v1
    spec:
      containers:
      - name: accounting-core
        image: accounting-system/accounting-core:v1.0.0
        ports:
        - containerPort: 8001
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        - name: KAFKA_BROKERS
          value: "kafka:9092"
        - name: REDIS_URL
          value: "redis://redis:6379"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8001
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: accounting-core-config

---
apiVersion: v1
kind: Service
metadata:
  name: accounting-core-service
  namespace: accounting-system
  labels:
    app: accounting-core
spec:
  selector:
    app: accounting-core
  ports:
  - name: http
    port: 8001
    targetPort: 8001
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: accounting-core-hpa
  namespace: accounting-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: accounting-core-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### Helm chart Ð´Ð»Ñ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ
```yaml
# helm/accounting-system/Chart.yaml
apiVersion: v2
name: accounting-system
description: Georgian IFRS-compliant accounting system
type: application
version: 1.0.0
appVersion: "1.0.0"

dependencies:
  - name: postgresql
    version: 12.1.9
    repository: https://charts.bitnami.com/bitnami
  - name: redis
    version: 17.3.7
    repository: https://charts.bitnami.com/bitnami
  - name: kafka
    version: 20.0.6
    repository: https://charts.bitnami.com/bitnami
```

```yaml
# helm/accounting-system/values.yaml
global:
  imageRegistry: "registry.accounting-system.com"
  imagePullSecrets: []

accountingCore:
  enabled: true
  image:
    repository: accounting-core
    tag: "v1.0.0"
  replicaCount: 3
  resources:
    requests:
      memory: 256Mi
      cpu: 250m
    limits:
      memory: 512Mi
      cpu: 500m
  
taxService:
  enabled: true
  image:
    repository: tax-service
    tag: "v1.0.0"
  replicaCount: 2
  georgianTax:
    rsApiUrl: "https://api.rs.ge"
    vatRate: 0.18

postgresql:
  enabled: true
  auth:
    postgresPassword: "secure-password"
    database: "accounting"
  primary:
    persistence:
      size: 100Gi
      storageClass: "fast-ssd"

redis:
  enabled: true
  auth:
    enabled: true
    password: "redis-password"

kafka:
  enabled: true
  replicaCount: 3
  persistence:
    size: 50Gi

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "1000"
  hosts:
    - host: api.accounting-system.ge
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: accounting-system-tls
      hosts:
        - api.accounting-system.ge
```

### Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¤Ð°Ð·Ñ‹ 3:
- âœ… 9 Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… Ð¼Ð¸ÐºÑ€Ð¾ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
- âœ… API Gateway Ñ authentication/authorization
- âœ… Service mesh Ñ Istio
- âœ… Container orchestration Ð² Kubernetes
- âœ… Auto-scaling Ð¸ self-healing
- âœ… 90% Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð°Ð¼Ð¸ Ð²ÑÐµÑ… ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²

---

## ðŸš€ Ð¤Ð°Ð·Ð° 4: Cloud Native Ð¸ Advanced Features (ÐœÐµÑÑÑ†Ñ‹ 15-18)

### Ð¦ÐµÐ»Ð¸:
- Cloud-native deployment
- Machine Learning Ð´Ð»Ñ fraud detection
- Advanced analytics
- Multi-tenant architecture

### 4.2 Machine Learning Ð´Ð»Ñ Fraud Detection (ÐœÐµÑÑÑ† 16)

#### ML Pipeline Ð´Ð»Ñ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸ Ð² Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑÑ…
```python
# ml/fraud_detection/models.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
from typing import Dict, List, Tuple
import asyncio

class TransactionFraudDetector:
    def __init__(self):
        self.isolation_forest = IsolationForest(
            contamination=0.1,  # 10% Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹ Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ÑÑ
            random_state=42,
            n_estimators=200
        )
        self.scaler = StandardScaler()
        self.feature_columns = [
            'amount_gel', 'hour_of_day', 'day_of_week', 'days_since_last_transaction',
            'amount_zscore', 'frequency_last_week', 'account_age_days',
            'transaction_count_today', 'average_transaction_amount'
        ]
        self.is_trained = False
    
    def extract_features(self, transactions: List[Dict]) -> pd.DataFrame:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð´Ð»Ñ ML Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        df = pd.DataFrame(transactions)
        
        # Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        df['timestamp'] = pd.to_datetime(df['created_at'])
        df['hour_of_day'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        
        # ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹
        df = df.sort_values(['user_id', 'timestamp'])
        df['days_since_last_transaction'] = df.groupby('user_id')['timestamp'].diff().dt.total_seconds() / (24 * 3600)
        df['days_since_last_transaction'].fillna(0, inplace=True)
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸
        user_stats = df.groupby('user_id').agg({
            'amount_gel': ['mean', 'std', 'count'],
            'timestamp': ['min']
        }).reset_index()
        
        user_stats.columns = ['user_id', 'avg_amount', 'std_amount', 'transaction_count', 'first_transaction']
        user_stats['account_age_days'] = (pd.Timestamp.now() - user_stats['first_transaction']).dt.total_seconds() / (24 * 3600)
        
        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ñ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
        df = df.merge(user_stats[['user_id', 'avg_amount', 'std_amount', 'account_age_days']], on='user_id')
        
        # Z-score Ð´Ð»Ñ ÑÑƒÐ¼Ð¼Ñ‹
        df['amount_zscore'] = np.abs((df['amount_gel'] - df['avg_amount']) / (df['std_amount'] + 1e-6))
        
        # Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð° Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÑŽÑŽ Ð½ÐµÐ´ÐµÐ»ÑŽ
        df['frequency_last_week'] = df.groupby('user_id')['timestamp'].transform(
            lambda x: x.rolling('7D').count()
        )
        
        # ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹ ÑÐµÐ³Ð¾Ð´Ð½Ñ
        df['transaction_count_today'] = df.groupby(['user_id', df['timestamp'].dt.date]).cumcount() + 1
        
        return df[self.feature_columns].fillna(0)
    
    async def train(self, training_data: List[Dict]):
        """ÐžÐ±ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð° Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        features_df = self.extract_features(training_data)
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
        features_scaled = self.scaler.fit_transform(features_df)
        
        # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        self.isolation_forest.fit(features_scaled)
        self.is_trained = True
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        joblib.dump(self.isolation_forest, 'models/fraud_detector.joblib')
        joblib.dump(self.scaler, 'models/fraud_scaler.joblib')
        
        print(f"Model trained on {len(training_data)} transactions")
    
    async def predict_fraud_probability(self, transaction: Dict) -> float:
        """ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°"""
        if not self.is_trained:
            await self.load_model()
        
        # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð¹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸
        features_df = self.extract_features([transaction])
        features_scaled = self.scaler.transform(features_df)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ anomaly score (-1 = Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ñ, 1 = Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ)
        anomaly_score = self.isolation_forest.decision_function(features_scaled)[0]
        
        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ (0-1)
        fraud_probability = max(0, min(1, (1 - anomaly_score) / 2))
        
        return fraud_probability
    
    async def load_model(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ"""
        try:
            self.isolation_forest = joblib.load('models/fraud_detector.joblib')
            self.scaler = joblib.load('models/fraud_scaler.joblib')
            self.is_trained = True
        except FileNotFoundError:
            print("Pre-trained model not found. Training new model...")
            # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð¾Ð±ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ

class FraudDetectionService:
    def __init__(self):
        self.detector = TransactionFraudDetector()
        self.fraud_threshold = 0.7  # ÐŸÐ¾Ñ€Ð¾Ð³ Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°
    
    async def analyze_transaction(self, transaction: Dict) -> Dict:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°"""
        fraud_probability = await self.detector.predict_fraud_probability(transaction)
        
        risk_level = "low"
        if fraud_probability > self.fraud_threshold:
            risk_level = "high"
        elif fraud_probability > 0.4:
            risk_level = "medium"
        
        return {
            "transaction_id": transaction["id"],
            "fraud_probability": fraud_probability,
            "risk_level": risk_level,
            "requires_review": fraud_probability > self.fraud_threshold,
            "analysis_timestamp": datetime.utcnow().isoformat()
        }
    
    async def handle_transaction_created_event(self, event_data: Dict):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ Ð´Ð»Ñ ML Ð°Ð½Ð°Ð»Ð¸Ð·Ð°"""
        transaction = event_data["event_data"]
        analysis_result = await self.analyze_transaction(transaction)
        
        if analysis_result["requires_review"]:
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð°Ð»ÐµÑ€Ñ‚ Ð´Ð»Ñ Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
            await self.send_fraud_alert(analysis_result)
            
            # Ð—Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸ÑŽ Ð´Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
            await self.flag_transaction_for_review(transaction["id"])
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        await self.save_fraud_analysis(analysis_result)
    
    async def send_fraud_alert(self, analysis_result: Dict):
        """ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ Ð¾ Ð¿Ð¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
        alert = {
            "alert_type": "fraud_detection",
            "severity": "high",
            "transaction_id": analysis_result["transaction_id"],
            "fraud_probability": analysis_result["fraud_probability"],
            "message": f"High fraud probability detected: {analysis_result['fraud_probability']:.2%}"
        }
        
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· notification service
        await self.notification_service.send_alert(alert)
```

#### Real-time ML inference
```python
# ml/realtime_inference/inference_service.py
from kafka import KafkaConsumer, KafkaProducer
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging

class RealtimeMLInferenceService:
    def __init__(self):
        self.fraud_detector = FraudDetectionService()
        self.consumer = KafkaConsumer(
            'accounting.transactioncreated',
            bootstrap_servers=['kafka:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id='ml-fraud-detection'
        )
        self.producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8')
        )
        self.executor = ThreadPoolExecutor(max_workers=10)
    
    async def start_processing(self):
        """Ð—Ð°Ð¿ÑƒÑÐº real-time Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹"""
        logger.info("Starting ML inference service...")
        
        loop = asyncio.get_event_loop()
        
        for message in self.consumer:
            # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ Ð´Ð»Ñ Ð½ÐµÐ±Ð»Ð¾ÐºÐ¸Ñ€ÑƒÑŽÑ‰ÐµÐ¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹
            future = loop.run_in_executor(
                self.executor,
                self.process_transaction_message,
                message.value
            )
            
            # ÐÐµ Ð¶Ð´ÐµÐ¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
            asyncio.create_task(self.handle_inference_result(future))
    
    def process_transaction_message(self, event_data: Dict) -> Dict:
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¾ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
        try:
            analysis_result = asyncio.run(
                self.fraud_detector.analyze_transaction(event_data)
            )
            return analysis_result
        except Exception as e:
            logger.error(f"ML inference failed: {e}")
            return {
                "error": str(e),
                "transaction_id": event_data.get("aggregate_id")
            }
    
    async def handle_inference_result(self, future):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ML inference"""
        try:
            result = await future
            
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð² Kafka
            self.producer.send('ml.fraud-analysis-result', result)
            
            if result.get("requires_review"):
                # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð² high-priority Ñ‚Ð¾Ð¿Ð¸Ðº Ð´Ð»Ñ Ð½ÐµÐ¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
                self.producer.send('alerts.high-priority', {
                    "type": "fraud_detection",
                    "data": result
                })
                
        except Exception as e:
            logger.error(f"Failed to handle inference result: {e}")
```

### 4.3 Advanced Analytics Ð¸ BI (ÐœÐµÑÑÑ† 17)

#### Real-time Analytics Ñ ClickHouse
```python
# analytics/clickhouse_client.py
from clickhouse_driver import Client
from typing import List, Dict
import asyncio

class ClickHouseAnalytics:
    def __init__(self, host='clickhouse', port=9000):
        self.client = Client(host=host, port=port)
        self.setup_tables()
    
    def setup_tables(self):
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸"""
        # Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° Ð´Ð»Ñ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹ (Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°)
        self.client.execute('''
            CREATE TABLE IF NOT EXISTS transactions_analytics (
                transaction_id UUID,
                transaction_date Date,
                transaction_timestamp DateTime,
                company_id UUID,
                user_id UUID,
                total_amount Decimal(15, 2),
                currency_code String,
                account_debit_id UUID,
                account_credit_id UUID,
                account_debit_type String,
                account_credit_type String,
                is_approved UInt8,
                is_posted UInt8,
                created_at DateTime
            ) ENGINE = MergeTree()
            PARTITION BY toYYYYMM(transaction_date)
            ORDER BY (company_id, transaction_date, transaction_id)
        ''')
        
        # ÐœÐ°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ real-time Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸
        self.client.execute('''
            CREATE MATERIALIZED VIEW IF NOT EXISTS daily_transactions_mv
            TO daily_transactions_summary
            AS SELECT
                company_id,
                transaction_date,
                count() as transaction_count,
                sum(total_amount) as total_amount,
                avg(total_amount) as avg_amount,
                countIf(is_posted = 1) as posted_count,
                sumIf(total_amount, is_posted = 1) as posted_amount
            FROM transactions_analytics
            GROUP BY company_id, transaction_date
        ''')
        
        # Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° Ð´Ð»Ñ ÐšÐŸÐŸ (ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸)
        self.client.execute('''
            CREATE TABLE IF NOT EXISTS kpi_metrics (
                company_id UUID,
                metric_date Date,
                metric_name String,
                metric_value Decimal(15, 2),
                metric_currency String DEFAULT 'GEL',
                created_at DateTime DEFAULT now()
            ) ENGINE = ReplacingMergeTree(created_at)
            PARTITION BY toYYYYMM(metric_date)
            ORDER BY (company_id, metric_date, metric_name)
        ''')
    
    async def insert_transaction(self, transaction_data: Dict):
        """Ð’ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸"""
        await asyncio.to_thread(
            self.client.execute,
            'INSERT INTO transactions_analytics VALUES',
            [transaction_data]
        )
    
    async def get_financial_kpis(self, company_id: str, start_date: str, end_date: str) -> Dict:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ðµ ÐšÐŸÐŸ Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´"""
        query = '''
            SELECT
                -- ÐžÐ±Ð¾Ñ€Ð¾Ñ‚
                sum(total_amount) as total_revenue,
                count() as transaction_count,
                avg(total_amount) as avg_transaction_amount,
                
                -- ÐŸÐ¾ Ñ‚Ð¸Ð¿Ð°Ð¼ ÑÑ‡ÐµÑ‚Ð¾Ð²
                sumIf(total_amount, account_credit_type = 'REVENUE') as revenue,
                sumIf(total_amount, account_debit_type = 'EXPENSE') as expenses,
                
                -- ÐÐºÑ‚Ð¸Ð²Ñ‹ Ð¸ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°
                sumIf(total_amount, account_debit_type = 'ASSET') as total_assets_increase,
                sumIf(total_amount, account_credit_type = 'LIABILITY') as total_liabilities_increase,
                
                -- Ð ÐµÐ½Ñ‚Ð°Ð±ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ
                (sumIf(total_amount, account_credit_type = 'REVENUE') - 
                 sumIf(total_amount, account_debit_type = 'EXPENSE')) as profit_loss
                
            FROM transactions_analytics
            WHERE company_id = %(company_id)s
            AND transaction_date BETWEEN %(start_date)s AND %(end_date)s
            AND is_posted = 1
        '''
        
        result = await asyncio.to_thread(
            self.client.execute,
            query,
            {'company_id': company_id, 'start_date': start_date, 'end_date': end_date}
        )
        
        return {
            'total_revenue': float(result[0][0] or 0),
            'transaction_count': result[0][1],
            'avg_transaction_amount': float(result[0][2] or 0),
            'revenue': float(result[0][3] or 0),
            'expenses': float(result[0][4] or 0),
            'total_assets_increase': float(result[0][5] or 0),
            'total_liabilities_increase': float(result[0][6] or 0),
            'profit_loss': float(result[0][7] or 0)
        }

class RealTimeAnalyticsService:
    def __init__(self):
        self.clickhouse = ClickHouseAnalytics()
        self.redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)
    
    async def handle_transaction_posted_event(self, event_data: Dict):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð¿Ñ€Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸"""
        transaction = event_data['event_data']
        
        # Ð’ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð² ClickHouse Ð´Ð»Ñ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ð¾Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸
        analytics_record = {
            'transaction_id': event_data['aggregate_id'],
            'transaction_date': transaction['transaction_date'][:10],
            'transaction_timestamp': transaction['transaction_date'],
            'company_id': transaction['company_id'],
            'user_id': transaction['created_by'],
            'total_amount': float(transaction['total_amount']),
            'currency_code': transaction.get('currency_code', 'GEL'),
            'is_posted': 1,
            'created_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        await self.clickhouse.insert_transaction(analytics_record)
        
        # ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ real-time Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð² Redis
        await self.update_realtime_metrics(transaction)
    
    async def update_realtime_metrics(self, transaction: Dict):
        """ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸"""
        company_id = transaction['company_id']
        date_key = transaction['transaction_date'][:10]
        
        # Ð¡Ñ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ¸ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹
        await self.redis_client.hincrby(
            f"metrics:daily:{company_id}:{date_key}",
            "transaction_count",
            1
        )
        
        # Ð¡ÑƒÐ¼Ð¼Ð° Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹
        await self.redis_client.hincrbyfloat(
            f"metrics:daily:{company_id}:{date_key}",
            "total_amount",
            float(transaction['total_amount'])
        )
        
        # Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ TTL Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (30 Ð´Ð½ÐµÐ¹)
        await self.redis_client.expire(
            f"metrics:daily:{company_id}:{date_key}",
            30 * 24 * 3600
        )
```

#### Business Intelligence Dashboard
```python
# analytics/dashboard_api.py
from fastapi import FastAPI, Depends, Query
from datetime import datetime, timedelta
import plotly.graph_objects as go
import plotly.express as px

class BIDashboardAPI:
    def __init__(self):
        self.app = FastAPI(title="Accounting Analytics Dashboard")
        self.clickhouse = ClickHouseAnalytics()
        self.setup_routes()
    
    def setup_routes(self):
        @self.app.get("/api/v1/dashboard/overview")
        async def get_overview(
            company_id: str,
            period: str = Query("30d", regex="^(7d|30d|90d|1y)$")
        ):
            """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ð±Ð·Ð¾Ñ€Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ dashboard"""
            end_date = datetime.now().date()
            
            if period == "7d":
                start_date = end_date - timedelta(days=7)
            elif period == "30d":
                start_date = end_date - timedelta(days=30)
            elif period == "90d":
                start_date = end_date - timedelta(days=90)
            else:  # 1y
                start_date = end_date - timedelta(days=365)
            
            kpis = await self.clickhouse.get_financial_kpis(
                company_id, str(start_date), str(end_date)
            )
            
            # Ð¢Ñ€ÐµÐ½Ð´ Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´
            trend_data = await self.get_trend_data(company_id, start_date, end_date)
            
            return {
                "period": period,
                "kpis": kpis,
                "trends": trend_data,
                "generated_at": datetime.utcnow().isoformat()
            }
        
        @self.app.get("/api/v1/dashboard/profit-loss-chart")
        async def get_profit_loss_chart(company_id: str, period: str = "30d"):
            """Ð“Ñ€Ð°Ñ„Ð¸Ðº Ð¿Ñ€Ð¸Ð±Ñ‹Ð»Ð¸ Ð¸ ÑƒÐ±Ñ‹Ñ‚ÐºÐ¾Ð²"""
            # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ClickHouse
            query = '''
                SELECT
                    transaction_date,
                    sumIf(total_amount, account_credit_type = 'REVENUE') as revenue,
                    sumIf(total_amount, account_debit_type = 'EXPENSE') as expenses
                FROM transactions_analytics
                WHERE company_id = %(company_id)s
                AND transaction_date >= today() - 30
                GROUP BY transaction_date
                ORDER BY transaction_date
            '''
            
            data = await asyncio.to_thread(
                self.clickhouse.client.execute,
                query,
                {'company_id': company_id}
            )
            
            # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Plotly Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°
            dates = [row[0] for row in data]
            revenues = [float(row[1] or 0) for row in data]
            expenses = [float(row[2] or 0) for row in data]
            profit = [r - e for r, e in zip(revenues, expenses)]
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=dates,
                y=revenues,
                mode='lines+markers',
                name='Ð”Ð¾Ñ…Ð¾Ð´Ñ‹',
                line=dict(color='green')
            ))
            
            fig.add_trace(go.Scatter(
                x=dates,
                y=expenses,
                mode='lines+markers',
                name='Ð Ð°ÑÑ…Ð¾Ð´Ñ‹',
                line=dict(color='red')
            ))
            
            fig.add_trace(go.Scatter(
                x=dates,
                y=profit,
                mode='lines+markers',
                name='ÐŸÑ€Ð¸Ð±Ñ‹Ð»ÑŒ',
                line=dict(color='blue'),
                fill='tonexty'
            ))
            
            fig.update_layout(
                title="Ð”Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ° Ð¿Ñ€Ð¸Ð±Ñ‹Ð»Ð¸ Ð¸ ÑƒÐ±Ñ‹Ñ‚ÐºÐ¾Ð²",
                xaxis_title="Ð”Ð°Ñ‚Ð°",
                yaxis_title="Ð¡ÑƒÐ¼Ð¼Ð° (áƒšáƒáƒ áƒ˜)",
                hovermode='x unified'
            )
            
            return fig.to_json()
        
        @self.app.get("/api/v1/dashboard/account-balances")
        async def get_account_balances(company_id: str):
            """Ð‘Ð°Ð»Ð°Ð½ÑÑ‹ Ð¿Ð¾ Ñ‚Ð¸Ð¿Ð°Ð¼ ÑÑ‡ÐµÑ‚Ð¾Ð²"""
            query = '''
                SELECT
                    account_type,
                    sum(current_balance) as total_balance
                FROM account_balances ab
                JOIN accounts a ON ab.account_id = a.id
                WHERE a.company_id = %(company_id)s
                GROUP BY account_type
                ORDER BY total_balance DESC
            '''
            
            # Ð­Ñ‚Ð¾ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ PostgreSQL Ð±Ð°Ð·Ðµ, Ð½Ðµ ClickHouse
            # Ð—Ð´ÐµÑÑŒ Ð½ÑƒÐ¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚
            
            return {
                "account_balances": [
                    {"type": "ÐÐºÑ‚Ð¸Ð²Ñ‹", "balance": 150000.00},
                    {"type": "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð°", "balance": 75000.00},
                    {"type": "ÐšÐ°Ð¿Ð¸Ñ‚Ð°Ð»", "balance": 75000.00}
                ]
            }
```

### 4.4 Multi-tenant Architecture (ÐœÐµÑÑÑ† 18)

#### Tenant isolation
```python
# tenancy/tenant_context.py
from contextvars import ContextVar
from typing import Optional
import uuid

# ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð°Ñ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ‚ÐµÐ½Ð°Ð½Ñ‚Ð°
current_tenant: ContextVar[Optional[str]] = ContextVar('current_tenant', default=None)

class TenantContext:
    def __init__(self, tenant_id: str):
        self.tenant_id = tenant_id
        self.token = None
    
    def __enter__(self):
        self.token = current_tenant.set(self.tenant_id)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.token:
            current_tenant.reset(self.token)

class TenantAwareRepository:
    """Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ tenant-aware Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸ÐµÐ²"""
    
    def __init__(self, db_pool):
        self.db_pool = db_pool
    
    def get_tenant_id(self) -> str:
        tenant_id = current_tenant.get()
        if not tenant_id:
            raise ValueError("No tenant context set")
        return tenant_id
    
    async def execute_query(self, query: str, params: list = None, tenant_filter: bool = True):
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¿Ð¾ tenant"""
        if tenant_filter and "WHERE" in query.upper():
            # Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ tenant_id
            query = query.replace("WHERE", f"WHERE tenant_id = %s AND", 1)
            params = [self.get_tenant_id()] + (params or [])
        elif tenant_filter:
            # Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ WHERE clause ÐµÑÐ»Ð¸ ÐµÐ³Ð¾ Ð½ÐµÑ‚
            if "ORDER BY" in query.upper():
                query = query.replace("ORDER BY", "WHERE tenant_id = %s ORDER BY", 1)
            else:
                query += " WHERE tenant_id = %s"
            params = (params or []) + [self.get_tenant_id()]
        
        async with self.db_pool.acquire() as conn:
            return await conn.fetch(query, *params)

class AccountRepository(TenantAwareRepository):
    async def get_all_accounts(self) -> List[Account]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð²ÑÐµ ÑÑ‡ÐµÑ‚Ð° Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ‚ÐµÐ½Ð°Ð½Ñ‚Ð°"""
        query = """
            SELECT id, code, name, account_type, parent_id, is_active
            FROM accounts
            ORDER BY code
        """
        rows = await self.execute_query(query)
        return [Account(**dict(row)) for row in rows]
    
    async def create_account(self, account_data: AccountCreate) -> Account:
        """Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ ÑÑ‡ÐµÑ‚ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ‚ÐµÐ½Ð°Ð½Ñ‚Ð°"""
        query = """
            INSERT INTO accounts (id, tenant_id, code, name, account_type, parent_id, is_active)
            VALUES ($1, $2, $3, $4, $5, $6, $7)
            RETURNING *
        """
        account_id = uuid.uuid4()
        tenant_id = self.get_tenant_id()
        
        row = await self.execute_query(
            query,
            [account_id, tenant_id, account_data.code, account_data.name, 
             account_data.account_type, account_data.parent_id, True],
            tenant_filter=False  # Ð£Ð¶Ðµ Ð´Ð¾Ð±Ð°Ð²Ð¸Ð»Ð¸ tenant_id Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ
        )
        return Account(**dict(row[0]))

# Middleware Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ tenant Ð¸Ð· Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
class TenantMiddleware:
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            # Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ tenant_id Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ° Ð¸Ð»Ð¸ JWT Ñ‚Ð¾ÐºÐµÐ½Ð°
            headers = dict(scope["headers"])
            tenant_id = self.extract_tenant_id(headers)
            
            if tenant_id:
                with TenantContext(tenant_id):
                    await self.app(scope, receive, send)
            else:
                # Ð’ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ð¾ÑˆÐ¸Ð±ÐºÑƒ ÐµÑÐ»Ð¸ tenant Ð½Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½
                response = Response(
                    content="Tenant ID required",
                    status_code=400
                )
                await response(scope, receive, send)
        else:
            await self.app(scope, receive, send)
    
    def extract_tenant_id(self, headers) -> Optional[str]:
        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
        tenant_header = headers.get(b'x-tenant-id')
        if tenant_header:
            return tenant_header.decode()
        
        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¸Ð· JWT Ñ‚Ð¾ÐºÐµÐ½Ð°
        auth_header = headers.get(b'authorization')
        if auth_header and auth_header.startswith(b'Bearer '):
            token = auth_header[7:].decode()
            try:
                payload = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
                return payload.get('tenant_id')
            except:
                pass
        
        return None
```

#### Database schema Ð´Ð»Ñ multi-tenancy
```sql
-- migration: add_tenant_support.sql

-- Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ tenant_id ÐºÐ¾ Ð²ÑÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ð¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°Ð¼
ALTER TABLE companies ADD COLUMN IF NOT EXISTS tenant_id UUID;
ALTER TABLE users ADD COLUMN IF NOT EXISTS tenant_id UUID;
ALTER TABLE accounts ADD COLUMN IF NOT EXISTS tenant_id UUID;
ALTER TABLE journal_entries ADD COLUMN IF NOT EXISTS tenant_id UUID;
ALTER TABLE invoices ADD COLUMN IF NOT EXISTS tenant_id UUID;
ALTER TABLE products ADD COLUMN IF NOT EXISTS tenant_id UUID;
ALTER TABLE employees ADD COLUMN IF NOT EXISTS tenant_id UUID;

-- Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ñ‚ÐµÐ½Ð°Ð½Ñ‚Ð¾Ð²
CREATE TABLE IF NOT EXISTS tenants (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    subdomain VARCHAR(100) UNIQUE,
    plan VARCHAR(50) DEFAULT 'basic',
    max_users INTEGER DEFAULT 10,
    max_companies INTEGER DEFAULT 1,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Georgian-specific settings
    default_currency CHAR(3) DEFAULT 'GEL',
    vat_rate DECIMAL(5,4) DEFAULT 0.1800,
    tax_period VARCHAR(20) DEFAULT 'monthly',
    
    -- Feature flags
    features JSONB DEFAULT '{
        "advanced_reporting": false,
        "api_access": false,
        "multi_currency": true,
        "audit_trail": true,
        "ml_fraud_detection": false
    }'::jsonb
);

-- ÐŸÐ¾Ð´Ð¿Ð¸ÑÐºÐ¸ Ñ‚ÐµÐ½Ð°Ð½Ñ‚Ð¾Ð²
CREATE TABLE tenant_subscriptions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL REFERENCES tenants(id),
    plan_name VARCHAR(50) NOT NULL,
    started_at TIMESTAMPTZ NOT NULL,
    expires_at TIMESTAMPTZ,
    is_active BOOLEAN DEFAULT TRUE,
    monthly_price DECIMAL(10,2),
    currency CHAR(3) DEFAULT 'GEL',
    
    -- ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ð»Ð°Ð½Ð°
    limits JSONB DEFAULT '{
        "max_users": 10,
        "max_companies": 1,
        "max_transactions_per_month": 1000,
        "storage_gb": 10
    }'::jsonb,
    
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¸Ð½Ð´ÐµÐºÑÑ‹
DROP INDEX IF EXISTS idx_accounts_code;
CREATE INDEX idx_accounts_code_tenant ON accounts(tenant_id, code);

DROP INDEX IF EXISTS idx_journal_entries_account;
CREATE INDEX idx_journal_entries_account_tenant ON journal_entries(tenant_id, account_id, transaction_date);

-- Row Level Security (RLS) Ð´Ð»Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð·Ð¾Ð»ÑÑ†Ð¸Ð¸
ALTER TABLE accounts ENABLE ROW LEVEL SECURITY;
ALTER TABLE journal_entries ENABLE ROW LEVEL SECURITY;
ALTER TABLE invoices ENABLE ROW LEVEL SECURITY;

-- ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸ RLS
CREATE POLICY accounts_tenant_isolation ON accounts
    FOR ALL TO authenticated_users
    USING (tenant_id = current_setting('app.current_tenant_id')::uuid);

CREATE POLICY journal_entries_tenant_isolation ON journal_entries.1 Cloud Deployment (ÐœÐµÑÑÑ† 15)

#### Infrastructure as Code Ñ Terraform
```hcl
# terraform/main.tf
provider "google" {
  project = var.project_id
  region  = var.region
}

# GKE Cluster
resource "google_container_cluster" "accounting_cluster" {
  name     = "accounting-system"
  location = var.region
  
  remove_default_node_pool = true
  initial_node_count       = 1
  
  network    = google_compute_network.vpc.name
  subnetwork = google_compute_subnetwork.subnet.name
  
  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }
  
  workload_identity_config {
    workload_pool = "${var.project_id}.svc.id.goog"
  }
  
  addons_config {
    istio_config {
      disabled = false
      auth     = "AUTH_MUTUAL_TLS"
    }
  }
}

# Node pools
resource "google_container_node_pool" "primary_nodes" {
  name       = "primary-node-pool"
  location   = var.region
  cluster    = google_container_cluster.accounting_cluster.name
  node_count = 3
  
  node_config {
    preemptible  = false
    machine_type = "e2-standard-4"
    
    service_account = google_service_account.gke_service_account.email
    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform"
    ]
    
    labels = {
      environment = var.environment
    }
    
    tags = ["accounting-system-node"]
  }
  
  autoscaling {
    min_node_count = 2
    max_node_count = 20
  }
  
  management {
    auto_repair  = true
    auto_upgrade = true
  }
}

# Cloud SQL (PostgreSQL)
resource "google_sql_database_instance" "accounting_db" {
  name             = "accounting-db-${var.environment}"
  database_version = "POSTGRES_14"
  region          = var.region
  
  settings {
    tier              = "db-standard-4"
    availability_type = "REGIONAL"
    disk_type         = "PD_SSD"
    disk_size         = 500
    disk_autoresize   = true
    
    backup_configuration {
      enabled                        = true
      start_time                     = "02:00"
      point_in_time_recovery_enabled = true
      transaction_log_retention_days = 7
      backup_retention_settings {
        retained_backups = 30
      }
    }
    
    ip_configuration {
      ipv4_enabled                                  = false
      private_network                               = google_compute_network.vpc.id
      enable_private_path_for_google_cloud_services = true
    }
    
    database_flags {
      name  = "max_connections"
      value = "1000"
    }
    
    insights_config {
      query_insights_enabled  = true
      query_string_length     = 1024
      record_application_tags = true
      record_client_address   = true
    }
  }
  
  deletion_protection = true
}

# Redis (Memorystore)
resource "google_redis_instance" "accounting_cache" {
  name           = "accounting-cache"
  tier           = "STANDARD_HA"
  memory_size_gb = 16
  region         = var.region
  
  authorized_network = google_compute_network.vpc.id
  
  redis_version = "REDIS_7_0"
  display_name  = "Accounting System Cache"
}
```

#### GitOps Ñ ArgoCD
```yaml
# argocd/applications/accounting-system.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: accounting-system
  namespace: argocd
spec:
  project: default
  
  source:
    repoURL: https://github.com/accounting-system/k8s-manifests
    targetRevision: HEAD
    path: environments/production
    
  destination:
    server: https://kubernetes.default.svc
    namespace: accounting-system
    
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
        
  revisionHistoryLimit: 10
```

### 4: Optional[AIOKafkaConsumer] = None
        self.handlers: Dict[str, List[Callable]] = {}
    
    async def start(self):
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8')
        )
        await self.producer.start()
    
    async def publish(self, event: DomainEvent):
        """ÐžÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ¾Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ"""
        topic = f"accounting.{event.event_type.lower()}"
        
        event_payload = {
            "event_id": str(event.event_id),
            "aggregate_id": str(event.aggregate_id),
            "event_type": event.event_type,
            "event_data": event.event_data,
            "version": event.version,
            "timestamp": event.timestamp.isoformat(),
            "metadata": event.metadata
        }
        
        await self.producer.send(topic, event_payload)
        
        logger.info(
            "Event published",
            event_type=event.event_type,
            aggregate_id=str(event.aggregate_id),
            topic=topic
        )
    
    async def subscribe(self, event_type: str, handler: Callable):
        """ÐŸÐ¾Ð´Ð¿Ð¸ÑÐ°Ñ‚ÑŒÑÑ Ð½Ð° Ñ‚Ð¸Ð¿ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ"""
        if event_type not in self.handlers:
            self.handlers[event_type] = []
        self.handlers[event_type].append(handler)
    
    async def start_consuming(self):
        """ÐÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹"""
        topics = [f"accounting.{event_type.lower()}" for event_type in self.handlers.keys()]
        
        self.consumer